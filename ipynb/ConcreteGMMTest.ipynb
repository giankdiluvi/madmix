{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682d365e",
   "metadata": {},
   "source": [
    "# GMM multivariate Concrete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c34928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sys,time\n",
    "sys.path.insert(1, '../src/')\n",
    "import gibbs\n",
    "from concrete import *\n",
    "import aux\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa08efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_lposterior(xd,w,mus,Sigmas,y,mu0,Sigma0,chol=None):\n",
    "    \"\"\"\n",
    "    Evaluate the log posterior density of a GMM (given data y)\n",
    "    \n",
    "    Inputs:\n",
    "        xd     : (N,B) array, labels (N = # of observations, B = Monte Carlo sample size)\n",
    "        w      : (K,B) array, weights (K = # of clusters)\n",
    "        mus    : (K,D,B) array, cluster means (D = dimension of observations)\n",
    "        Sigmas : (K,D,D,B) array, cluster covariances\n",
    "        y      : (N,D) array, observations\n",
    "        mu0    : (K,D,B) array, prior cluster means \n",
    "        Sigma0 : (K,D,D,B) array, prior cluster covariances\n",
    "    \n",
    "    Outputs:\n",
    "        lp     : (B,) array, posterior distribution log density up to normalizing constant\n",
    "    \"\"\"\n",
    "    N=xd.shape[0]\n",
    "    K,D,B=mus.shape\n",
    "    chol=np.linalg.cholesky(np.moveaxis(Sigmas,3,1)) if chol is None else chol # (K,B,D,D)\n",
    "    \n",
    "    #lp = stats.dirichlet(np.ones(K)).logpdf(w) # prior weights\n",
    "    lp = 0.6931471805599453*np.ones(B) # uninformative dirichlet value; prevents issues when sum(w)<1 but very close\n",
    "    for k in range(K):\n",
    "        std_mu = np.squeeze(np.matmul(chol[k,:,:,:],(mus[k,:,:]-mu0[k,:,None]).T[:,:,None]))\n",
    "        lp += stats.invwishart(df=N/K,scale=Sigma0[k,:,:]*N/K).logpdf(Sigmas[k,:,:,:]) # prior Sigma\n",
    "        lp += stats.multivariate_normal(mean=np.zeros(D),cov=np.eye(D)).logpdf(std_mu) # prior mu\n",
    "    # end for\n",
    "    \n",
    "    for n in range(N): lp += np.log(w[xd[n,:].astype(int),np.arange(B)]) # prior labels\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            std_y = np.squeeze(np.matmul(chol[k,:,:,:],(y[n,:,None]-mus[k,:,:]).T[:,:,None]))\n",
    "            tmplp = stats.multivariate_normal(mean=np.zeros(D),cov=np.eye(D)).logpdf(std_y) # likelihood\n",
    "            tmplp += np.log(w[k]) # likelihood\n",
    "            \n",
    "            idx = (xd[n,:]==k) # label n = cluster k\n",
    "            ll  = np.zeros(B)\n",
    "            ll[idx]=tmplp[idx]  # when xn=k, add gaussian lp\n",
    "            lp += ll\n",
    "        # end for\n",
    "    # end for\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24261053",
   "metadata": {},
   "source": [
    "## Penguin data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a045f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/24b0dzl557v5m0_0658q2cxr0000gn/T/ipykernel_78847/2807751474.py:3: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  std_penguins=(penguins-penguins.mean())/penguins.std() # normalize data\n",
      "/var/folders/k3/24b0dzl557v5m0_0658q2cxr0000gn/T/ipykernel_78847/2807751474.py:3: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  std_penguins=(penguins-penguins.mean())/penguins.std() # normalize data\n"
     ]
    }
   ],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins().dropna()\n",
    "std_penguins=(penguins-penguins.mean())/penguins.std() # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293e0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_dat=np.array(std_penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']])\n",
    "pg_true=np.squeeze(np.array(penguins[['species']]))\n",
    "pg_true[pg_true=='Adelie']=0\n",
    "pg_true[pg_true=='Gentoo']=1\n",
    "pg_true[pg_true=='Chinstrap']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09df116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D=pg_dat.shape\n",
    "K=3\n",
    "\n",
    "mu0=np.array([[-2.,1.,-1.,-1.],  # green\n",
    "             [1.,1.,-0.5,-0.5],  # purple\n",
    "             [1.,-1.5,1.5,2.]])  # blue\n",
    "sigma0=np.zeros((K,D,D))\n",
    "for k in range(K): sigma0[k,:,:]=0.5*np.eye(D)\n",
    "w0=np.ones(K)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d72c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "gibbs_path = '../examples/GMM/sockeye_run/penguin/'\n",
    "pred_x  = aux.pkl_load(gibbs_path+'pred_x')\n",
    "pred_w = aux.pkl_load(gibbs_path+'pred_w')\n",
    "pred_mu = aux.pkl_load(gibbs_path+'pred_mu')\n",
    "pred_sigma = aux.pkl_load(gibbs_path+'pred_sigma')\n",
    "\n",
    "# convert gibbs output to torch tensors\n",
    "xs_concrete = torch.from_numpy(pred_x)\n",
    "ws_concrete = torch.from_numpy(pred_w)\n",
    "mus_concrete = torch.from_numpy(pred_mu)\n",
    "sigmas_concrete = torch.from_numpy(pred_sigma)\n",
    "\n",
    "N,K,D = pred_x.shape[1], pred_mu.shape[1], pred_mu.shape[2] # 333, 3, 4\n",
    "tau0=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2d232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "#      settings        #\n",
    "########################\n",
    "########################\n",
    "temp=1.\n",
    "depth=10\n",
    "width=8\n",
    "\n",
    "max_iters=101\n",
    "lr=1e-5\n",
    "\n",
    "conc_sample=gmm_concrete_sample(xs_concrete,ws_concrete,mus_concrete,sigmas_concrete,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6671f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 2159.467\n",
      "iter 10: loss = 2091.535\n",
      "iter 20: loss = 2037.234\n",
      "iter 30: loss = 1994.536\n",
      "iter 40: loss = 1960.975\n",
      "iter 50: loss = 1934.192\n",
      "iter 60: loss = 1912.402\n",
      "iter 70: loss = 1894.458\n",
      "iter 80: loss = 1879.511\n",
      "iter 90: loss = 1866.915\n",
      "iter 100: loss = 1856.163\n"
     ]
    }
   ],
   "source": [
    "tmp_flow,tmp_loss=trainGMMRealNVP(\n",
    "    temp=temp,depth=depth,N=N,K=K,D=D,tau0=tau0,sample=conc_sample,width=width,max_iters=max_iters,lr=lr,seed=2023,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bd33130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40505072.1313283\n"
     ]
    }
   ],
   "source": [
    "sample_size=1000\n",
    "tmp_sample=tmp_flow.sample(sample_size)\n",
    "xd_pg,ws_pg,mus_pg,Sigmas_pg=concrete_gmm_unpack(tmp_sample,N,K,D)\n",
    "ws_pg,mus_pg,Sigmas_pg=ws_pg.detach().numpy(),mus_pg.detach().numpy(),Sigmas_pg.detach().numpy()\n",
    "ws_pg-=aux.LogSumExp(ws_pg)\n",
    "ws_pg=np.exp(ws_pg)\n",
    "xd_labels=np.argmax(xd_pg,axis=1)\n",
    "idx = np.zeros(sample_size, dtype=bool)\n",
    "for j in range(sample_size):\n",
    "    if np.sum(np.isinf(Sigmas_pg[:,:,:,j]))>0: continue\n",
    "    idx[j] = True\n",
    "# end for\n",
    "\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "llq = tmp_flow.log_prob(tmp_sample).detach().numpy()\n",
    "llp = gmm_lposterior(xd_labels[:,idx],ws_pg[:,idx],mus_pg[:,:,idx],Sigmas_pg[:,:,:,idx],pg_dat,mu0,sigma0,\n",
    "                     chol=np.linalg.cholesky(np.moveaxis(Sigmas_pg[:,:,:,idx],3,1)))\n",
    "print(np.mean(llp-llq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff9e0c",
   "metadata": {},
   "source": [
    "## Waveform data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "180cab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "waveform_dat=pd.read_table('https://hastie.su.domains/ElemStatLearn/datasets/waveform.train')\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(waveform_dat[waveform_dat.columns.difference(['row.names','y'])])\n",
    "waveform_pca=np.array(waveform_dat[waveform_dat.columns.difference(['row.names','y'])])@pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8858aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_true=np.squeeze(np.array(waveform_dat[['y']]))-1\n",
    "wf_dat=waveform_pca[:,:2]\n",
    "N,D=wf_dat.shape\n",
    "K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0d8d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial arrays\n",
    "mu0=np.array([[-3.,4.],  # blue\n",
    "              [ 5.,4.],  # purple \n",
    "              [ 0.,0.]]) # yellow\n",
    "sigma0=np.zeros((K,D,D))\n",
    "for k in range(K): sigma0[k,:,:]=5.*np.eye(D)\n",
    "w0=np.ones(K)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b31c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "gibbs_path = '../examples/GMM/sockeye_run/waveform/'\n",
    "pred_x  = aux.pkl_load(gibbs_path+'pred_x')\n",
    "pred_w = aux.pkl_load(gibbs_path+'pred_w')\n",
    "pred_mu = aux.pkl_load(gibbs_path+'pred_mu')\n",
    "pred_sigma = aux.pkl_load(gibbs_path+'pred_sigma')\n",
    "\n",
    "# convert gibbs output to torch tensors\n",
    "xs_concrete = torch.from_numpy(pred_x)\n",
    "ws_concrete = torch.from_numpy(pred_w)\n",
    "mus_concrete = torch.from_numpy(pred_mu)\n",
    "sigmas_concrete = torch.from_numpy(pred_sigma)\n",
    "\n",
    "N,K,D = pred_x.shape[1], pred_mu.shape[1], pred_mu.shape[2] # 300, 3, 2\n",
    "tau0=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d07d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "#      settings        #\n",
    "########################\n",
    "########################\n",
    "temp=1.\n",
    "depth=10\n",
    "width=8\n",
    "\n",
    "max_iters=101\n",
    "lr=1e-5\n",
    "\n",
    "conc_sample=gmm_concrete_sample(xs_concrete,ws_concrete,mus_concrete,sigmas_concrete,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20173db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 1788.605\n",
      "iter 10: loss = 1755.861\n",
      "iter 20: loss = 1728.707\n",
      "iter 30: loss = 1705.948\n",
      "iter 40: loss = 1686.476\n",
      "iter 50: loss = 1669.641\n",
      "iter 60: loss = 1654.796\n",
      "iter 70: loss = 1641.525\n",
      "iter 80: loss = 1629.609\n",
      "iter 90: loss = 1618.856\n",
      "iter 100: loss = 1609.125\n"
     ]
    }
   ],
   "source": [
    "tmp_flow,tmp_loss=trainGMMRealNVP(\n",
    "    temp=temp,depth=depth,N=N,K=K,D=D,tau0=tau0,sample=conc_sample,width=width,max_iters=max_iters,lr=lr,seed=2023,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf61d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-208710.63748522804\n"
     ]
    }
   ],
   "source": [
    "sample_size=1000\n",
    "tmp_sample=tmp_flow.sample(sample_size)\n",
    "xd_pg,ws_pg,mus_pg,Sigmas_pg=concrete_gmm_unpack(tmp_sample,N,K,D)\n",
    "ws_pg,mus_pg,Sigmas_pg=ws_pg.detach().numpy(),mus_pg.detach().numpy(),Sigmas_pg.detach().numpy()\n",
    "ws_pg-=aux.LogSumExp(ws_pg)\n",
    "ws_pg=np.exp(ws_pg)\n",
    "xd_labels=np.argmax(xd_pg,axis=1)\n",
    "idx = np.zeros(sample_size, dtype=bool)\n",
    "for j in range(sample_size):\n",
    "    if np.sum(np.isinf(Sigmas_pg[:,:,:,j]))>0: continue\n",
    "    idx[j] = True\n",
    "# end for\n",
    "\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "llq = tmp_flow.log_prob(tmp_sample).detach().numpy()\n",
    "llp = gmm_lposterior(xd_labels[:,idx],ws_pg[:,idx],mus_pg[:,:,idx],Sigmas_pg[:,:,:,idx],wf_dat,mu0,sigma0,\n",
    "                     chol=np.linalg.cholesky(np.moveaxis(Sigmas_pg[:,:,:,idx],3,1)))\n",
    "print(np.mean(llp-llq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
