{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996a4835",
   "metadata": {},
   "source": [
    "# MAD Mix GMM extension\n",
    "\n",
    "To handle multivariate data and learn weights and covariance matrices.\n",
    "\n",
    "## Model details\n",
    "\n",
    "We will use conjugate priors to make all the calculations easier.\n",
    "The discrete variables $x_{d,1:N}$ correspond to the cluster labels.\n",
    "The full conditionals are proportional to the weighted Gaussian pdf:\n",
    "$$\n",
    "    \\mathbb{P}(X_{d,n}=k \\,|\\,x_{d,-n},w_{1:K},\\mu_{1:K},\\Sigma_{1:K})\n",
    "    \\propto w_k\\phi_{\\mu_k,\\Sigma_k}(y_n)\n",
    "$$\n",
    "for $n=1,\\dots,N,\\,k=1,\\dots,K$.\n",
    "\n",
    "The continuous variables are slightly trickier.\n",
    "From the mean-field setting, we have that\n",
    "$$\n",
    "    w_{1:K}\\sim\\mathrm{Dirichlet}(N_1+1,\\dots,N_K+1)\n",
    "$$\n",
    "where $N_k=\\sum_n \\mathbf{1}(x_n=k)$ is the number of elements in cluster $k$.\n",
    "\n",
    "Within each cluster $k$, the mean and covariance follow a Gaussian-InverseWishart distribution:\n",
    "$$\n",
    "   \\mu_k\\,|\\,\\Sigma_k\\sim\\mathcal{N}(\\bar{y}_k,\\Sigma_k/N_k),\\\\\n",
    "   \\Sigma_k\\sim\\mathrm{IW}(S_k,N_k-D-1),\n",
    "$$\n",
    "where $\\bar{y}_k=N_k^{-1}\\sum_n y_n\\mathbf{1}(x_n=k)$ is the mean of elements in cluster $k$\n",
    "and $S_k=\\sum_n (y_n-\\bar{y}_k)(y_n-\\bar{y}_k)^\\top \\mathbf{1}(x_n=k)$ the \n",
    "corresponding (scaled) covariance.\n",
    "\n",
    "This results in a joint target (with fixed labels)\n",
    "of the form $p(w,\\mu,\\Sigma)=p(w)\\prod_k p(\\mu_k\\,|\\,\\Sigma_k)p(\\Sigma_k)$.\n",
    "\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "For the deterministic Hamiltonian move,\n",
    "we need the score function of the parameters $(w,\\mu,\\Sigma)$.\n",
    "Note that the score w.r.t. the weights will only depend on $p(w)$\n",
    "and the score w.r.t. the $k$th mean will only depend on $p(\\mu_k\\,|\\,\\Sigma_k)$.\n",
    "\n",
    "The score w.r.t. the weights is then\n",
    "$$\n",
    "    \\nabla_w \\log p(w,\\mu,\\Sigma)\n",
    "    =\\nabla_w \\log p(w)\n",
    "    =\\nabla_w \\sum_k N_k \\log w_k\n",
    "    =\\left(\\frac{N_1}{w_1},\\dots,\\frac{N_K}{w_K}\\right)^\\top.\n",
    "$$\n",
    "\n",
    "The score w.r.t. the means is \n",
    "$$\n",
    "    \\nabla_{\\mu_k} \\log p(w,\\mu,\\Sigma)\n",
    "    =\\nabla_{\\mu_k} \\log p(\\mu_k\\,|\\,\\Sigma)\n",
    "    =\\nabla_{\\mu_k} -\\frac{N_k}{2}(\\mu_k-\\bar{y}_k)\\top\\Sigma_k^{-1}(\\mu_k-\\bar{y}_k)\n",
    "    =-N_k\\Sigma_k^{-1}(\\mu_k-\\bar{y}_k).\n",
    "$$\n",
    "\n",
    "Finally, the score w.r.t. the covariances depends on both the mean pdf and the covariance pdf:\n",
    "$$\n",
    "    \\nabla_{\\Sigma_k} \\log p(w,\\mu,\\Sigma)\n",
    "    =\\nabla_{\\Sigma_k}\\log p(\\mu_k\\,|\\,\\Sigma_k) + \\log p(\\Sigma_k).\n",
    "$$\n",
    "The first term is\n",
    "$$\n",
    "    \\nabla_{\\Sigma_k}\\log p(\\mu_k\\,|\\,\\Sigma_k)\n",
    "    =\\nabla_{\\Sigma_k} -\\frac{1}{2}\\log|\\Sigma_k|\n",
    "    -\\frac{N_k}{2}(\\mu_k-\\bar{y}_k)\\top\\Sigma_k^{-1}(\\mu_k-\\bar{y}_k)\n",
    "    =-\\frac{1}{2}\\Sigma_k^{-\\top}-\\frac{N_k}{2}(\\mu_k-\\bar{y}_k)(\\mu_k-\\bar{y}_k)^\\top,\n",
    "$$\n",
    "where we used the identities $\\partial \\log|X|=X^{-\\top}$\n",
    "and $\\partial a^\\top X b=ab^\\top$ from the \n",
    "[matrixcookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n",
    "\n",
    "The second term is\n",
    "$$\n",
    "    \\nabla_{\\Sigma_k}\\log p(\\Sigma_k)\n",
    "    =\\nabla_{\\Sigma_k} -\\frac{N_k}{2}\\log|\\Sigma_k|-\\frac{1}{2}\\mathrm{tr}(S_k\\Sigma_k^{-1})\n",
    "    =-\\frac{N_k}{2}\\Sigma_k^{-\\top}+\\frac{1}{2}\\left(\\Sigma_k^{-1}S_k\\Sigma_k^{-1}\\right)^\\top,\n",
    "$$\n",
    "where we used the \n",
    "[pdf of the Inverse-Wishart distribution](https://en.wikipedia.org/wiki/Inverse-Wishart_distribution)\n",
    "and the identity $\\partial\\mathrm{tr}(AX^{-1}B)=-X^{-\\top}A^\\top B^\\top X^{-\\top}$\n",
    "from the [matrixcookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n",
    "\n",
    "Adding together these two expressions yields the score w.r.t. covariance:\n",
    "$$\n",
    "    \\nabla_{\\Sigma_k} \\log p(w,\\mu,\\Sigma)\n",
    "    =-\\frac{1}{2}(1+N_k)\\Sigma_k^{-\\top}\n",
    "    -\\frac{N_k}{2}(\\mu_k-\\bar{y}_k)(\\mu_k-\\bar{y}_k)^\\top\n",
    "    +\\frac{1}{2}\\left(\\Sigma_k^{-1}S_k\\Sigma_k^{-1}\\right)^\\top.\n",
    "$$\n",
    "\n",
    "## Covariance parametrization\n",
    "\n",
    "In practice, we don't want to deal with the covariance directly since it contains too many restrictions:\n",
    "it has to be symmetric and positive definite,\n",
    "and taking Hamiltonian steps can easily make the resulting matrix inadmissible.\n",
    "Instead, we follow\n",
    "[the Stan documentation on covariances](https://mc-stan.org/docs/reference-manual/covariance-matrices-1.html).\n",
    "Given a covariance matrix $\\Sigma$,\n",
    "let $\\Sigma=LL^\\top$ be its (unique) Cholesky decomposition.\n",
    "The matrix $L$ is lower triangular and has positive diagonal elements.\n",
    "Hence we only need to store the diagonal and lower diagonal elements&mdash;the rest are zero by default.\n",
    "To further remove the positiveness condition,\n",
    "define $H$ as a copy of $L$ but with the diagonal logged:\n",
    "$$\n",
    "    H_{ij}=\\begin{cases}\n",
    "        0,&i<j,\\\\\n",
    "        \\log L_{ij},&i=j,\\\\\n",
    "        L_{ij},&i>j.\n",
    "    \\end{cases}\n",
    "$$\n",
    "By taking steps in $H$-space, one is always guaranteed to get back a valid covariance matrix.\n",
    "To map from $H$ to $\\Sigma$,\n",
    "one simply exponentiates the diagonal to get $L$ and then sets $\\Sigma=LL^\\top$.\n",
    "\n",
    "Let $f$ be the function that maps $\\Sigma$ to $H$, so that $f(\\Sigma)=H$.\n",
    "The Jacobian of the transformation can be found in closed-form and\n",
    "the resulting density is given by\n",
    "$$\n",
    "    \\log p(H)=\\log p(\\Sigma)+\\sum_{d=1}^D (D-d+2)H_{dd} +D\\log2,\n",
    "$$\n",
    "where $D$ is the dimension of the space where the observations live, $y_n\\in\\mathbb{R}^D$.\n",
    "\n",
    "## Score w.r.t. $H$\n",
    "To take a Hamiltonian step in $H$ space, we need $\\nabla_H \\log p(H)$.\n",
    "We can use the formula we derived above and take gradient w.r.t. $H$.\n",
    "The third term does not depend on $H$.\n",
    "The second term is\n",
    "$$\n",
    "\\nabla_H \\sum_{d=1}^D (D-d+2)H_{dd}\n",
    "=\\nabla_H \\mathrm{Tr}(\\mathrm{diag((D+1)H_{11},\\dots,2H_{DD}})\n",
    "=\\nabla_H \\mathrm{Tr}(H\\mathrm{diag}((D-d+2)_{d=1}^D))\n",
    "=(\\mathrm{diag}((D-d+2)_{d=1}^D))^\\top,\n",
    "$$\n",
    "i.e., a diagonal matrix with the $d$th diagonal entry equal to $D-d+2$.\n",
    "Here we used the identity $\\partial\\mathrm{Tr}(XA)=A^\\top$\n",
    "from page 12 of the [matrixcookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n",
    "\n",
    "Using the chain rule, the first term can be diff'd via\n",
    "$$\n",
    "    \\nabla_H \\log p_\\Sigma(f^{-1}(H))\n",
    "    = \\nabla_H f^{-1}(H)\\nabla_\\Sigma \\log p_\\Sigma(f^{-1}(H)).\n",
    "$$\n",
    "The second factor is equal to $\\nabla_\\Sigma \\log p_\\Sigma(\\Sigma)$\n",
    "since $\\Sigma=f(H)$,\n",
    "and we derived that gradient above.\n",
    "\n",
    "The first factor is the Jacobian of the inverse transform $f$.\n",
    "Note that this is a $D^2\\times D^2$ matrix,\n",
    "so the second factor should be understood as a $D^2$ vector (a flattened matrix).\n",
    "We find this matrix by computing the Jacobian matrix of the transform\n",
    "from $H$ to $L$, say $J_1$,\n",
    "and multiplying it with the corresponding Jacobian\n",
    "of the map from $L$ to $LL^\\top=\\Sigma$, $J_2$.\n",
    "The first Jacobian matrix $J_1$ is diagonal with entries\n",
    "either 1 or $\\exp(H_{dd})$. Specifically,\n",
    "$$\n",
    "    J_1=\\mathrm{Diag}(\\exp(H_{11}),1,\\dots,1,\\exp(H_{22}),1,\\dots,1,\\exp(H_{DD}),1,\\dots,1).\n",
    "$$\n",
    "This is because \n",
    "$$\n",
    "    \\frac{\\partial L_{ij}}{\\partial H_{lk}}=\n",
    "    \\begin{cases}\n",
    "    \\exp{H_{lk}},& i=l=j=k,\\\\\n",
    "    1,&  i=l\\neq j=k,\\\\\n",
    "    0,&\\mathrm{o.w.}\n",
    "    \\end{cases}\n",
    "$$\n",
    "For $J_2$, we introduce the $D^2\\times D^2$ commutation matrix $K_D$.\n",
    "If $A$ is a $DxD$ matrix denote by $\\mathrm{vec}(A)$ the vectorized or flattened\n",
    "version of $A$, i.e., the $D^2$ vector obtained by stacking the columns of $A$\n",
    "one after the other.\n",
    "Then $K_D$ satisfies that $K_D\\mathrm{vec}(A)=\\mathrm{vec}(A^\\top)$.\n",
    "As shown in [here](https://math.stackexchange.com/questions/2158399/derivative-of-symmetric-positive-definite-matrix-w-r-t-to-its-lower-triangular),\n",
    "$$\n",
    "    J_2=\\frac{\\partial\\mathrm{vec}(LL^\\top)}{\\partial\\mathrm{vec}(L)}\n",
    "    =(L\\otimes I_D)+(L\\otimes I_D)K_D,\n",
    "$$\n",
    "where $\\otimes$ is the [Kroenecker product](https://en.wikipedia.org/wiki/Kronecker_product)\n",
    "and $I_D$ is the $D\\times D$ identity matrix.\n",
    "The tl;dr of the proof is decomposing\n",
    "$\\partial(LL^\\top)=\\partial L \\,L^\\top + L \\partial L^\\top$\n",
    "and then using properties of vectorization and Kroenecker products.\n",
    "Finally,\n",
    "$$\n",
    "    \\nabla_H f^{-1}(H)=J_2 J_1.\n",
    "$$\n",
    "\n",
    "Note: the MSc thesis \"On Jacobians connected with matrix variate random variables\"\n",
    "by Moses M. Njoroge (1988) contains a derivation of most of that $D^2\\times D^2$\n",
    "matrix in Figure 5 in the Appendix (page 84).\n",
    "Download [here](https://escholarship.mcgill.ca/downloads/0p096811t).\n",
    "The determinant Jacobian from above is also shown in page 39 (Theorem 4.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c10477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sys,time\n",
    "sys.path.insert(1, '../src/')\n",
    "import madmix\n",
    "import aux\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6524021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#   auxiliary fns  #\n",
    "####################\n",
    "####################\n",
    "def madmix_gmm_flatten(ws,mus,Hs):\n",
    "    \"\"\"\n",
    "    Flatten weights, meand, and logCholeskys into 2D array\n",
    "    \n",
    "    Inputs:\n",
    "        ws  : (K,B) array, weights\n",
    "        mus : (K,D,B) array, cluster means\n",
    "        Hs  : (K,D,D,B) array, cluster logCholesky matrices\n",
    "    \n",
    "    Outpus:\n",
    "        xc  : (K',B) array, flattened values\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + Kx(D+DChoose2) (covariances)\n",
    "    \"\"\"\n",
    "    K,D,B=mus.shape\n",
    "    \n",
    "    flat_mus=mus.reshape(K*D,B)\n",
    "    idx=np.tril_indices(D)\n",
    "    flat_Hs=Hs[:,idx[0],idx[1],:]                     # recover lower triangular entries\n",
    "    flat_Hs=flat_Hs.reshape(int(K*D*(1+0.5*(D-1))),B) # correct shape\n",
    "    return np.vstack((ws,flat_mus,flat_Hs))\n",
    "\n",
    "\n",
    "def madmix_gmm_unflatten(xc,K,D):\n",
    "    \"\"\"\n",
    "    Unflatten xc into weights, meand, and covariances\n",
    "    \n",
    "    Inputs:\n",
    "        xc  : (K',B) array, flattened values\n",
    "    \n",
    "    Outputs:\n",
    "        ws  : (K,B) array, weights\n",
    "        mus : (K,D,B) array, cluster means\n",
    "        Hs  : (K,D,D,B) array, cluster logCholesky matrices\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + Kx(D+DChoose2) (covariances)\n",
    "    \"\"\"\n",
    "    B=xc.shape[-1]\n",
    "    \n",
    "    # recover each flattened var\n",
    "    ws=xc[:K,:]\n",
    "    flat_mus=xc[K:(K*D+K),:]\n",
    "    flat_Hs=xc[(K*D+K):,:].reshape(K,int(D*(1+0.5*(D-1))),B)\n",
    "    \n",
    "    # unflatten separately\n",
    "    mus=flat_mus.reshape(K,D,B)\n",
    "    Hs=np.zeros((K,D,D,B))\n",
    "    idx=np.tril_indices(D)\n",
    "    Hs[:,idx[0],idx[1],:]=flat_Hs\n",
    "    \n",
    "    return ws,mus,Hs\n",
    "\n",
    "\n",
    "def HtoSigma(Hs):\n",
    "    \"\"\"\n",
    "    Transform logCholesky factors into covariance matrices\n",
    "    \n",
    "    Inputs:\n",
    "        Hs : (K,D,D,B) array, B observations of the K cluster logCholesky factors\n",
    "        \n",
    "    Outpus:\n",
    "        Sigmas : (K,D,D,B) array, B observations of the K cluster covariances\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=np.diag_indices(Hs.shape[1])\n",
    "    Ls=np.copy(Hs)\n",
    "    Ls[:,idx[0],idx[1],:]=np.exp(Hs[:,idx[0],idx[1],:])\n",
    "    Ls=np.moveaxis(Ls,3,1) # so matrices are stacked in last two axes for matmul\n",
    "    Sigmas=np.matmul(Ls,np.transpose(Ls,axes=(0,1,3,2)))\n",
    "    return np.moveaxis(Sigmas,1,3)\n",
    "\n",
    "def SigmatoH(Sigmas):\n",
    "    \"\"\"\n",
    "    Transform covariance matrices into logCholesky factors\n",
    "    \n",
    "    Inputs:\n",
    "        Sigmas : (K,D,D,B) array, B observations of the K cluster covariances\n",
    "        \n",
    "    Outpus:\n",
    "        Hs : (K,D,D,B) array, B observations of the K cluster logCholesky factors\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=np.diag_indices(Sigmas.shape[1])\n",
    "    Ls=np.linalg.cholesky(np.moveaxis(Sigmas,3,1))\n",
    "    Hs=np.copy(Ls)\n",
    "    Hs[:,:,idx[0],idx[1]]=np.log(Ls[:,:,idx[0],idx[1]])\n",
    "    return np.moveaxis(Hs,1,3)\n",
    "\n",
    "\n",
    "#####################\n",
    "# vec/vech matrices #\n",
    "#####################\n",
    "\n",
    "# elimination matrix (4,3)\n",
    "L2=np.zeros((3,4))\n",
    "L2[0,0],L2[1,1],L2[2,3]=1.,1.,1.\n",
    "\n",
    "# duplication matrix (3,4)\n",
    "D2=np.zeros((4,3))\n",
    "D2[0,0],D2[1,1],D2[2,1],D2[2,2]=1.,1.,1.,1.\n",
    "\n",
    "# commutation matrix (D,D)\n",
    "def comm_mat(D):\n",
    "    # copied from https://en.wikipedia.org/wiki/Commutation_matrix \n",
    "    # and modified by Gian Carlo Diluvi\n",
    "    # determine permutation applied by K\n",
    "    w = np.arange(D**2).reshape((D, D), order=\"F\").T.ravel(order=\"F\")\n",
    "\n",
    "    # apply this permutation to the rows (i.e. to each column) of identity matrix and return result\n",
    "    return np.eye(D**2)[w, :]\n",
    "\n",
    "def HtoSigmaJacobian(Hs):\n",
    "    \"\"\"\n",
    "    Calculate the Jacobian of the transformation H->Sigma\n",
    "    Since the transformation is DxD->DxD,\n",
    "    the Jacobian is a D**2xD**2 matrix \n",
    "    (to be multiplied with a vectorized matrix vec(grad log p(Sigma)))\n",
    "    \n",
    "    Inputs:\n",
    "        Hs       : (K,D,D,B) array, cluster logCholesky matrices\n",
    "        \n",
    "    Outputs:\n",
    "        jacobian : (K,D**2,D**2,B) array, Jacobian matrices\n",
    "    \"\"\"\n",
    "    K,D,B=Hs.shape[0],Hs.shape[1],Hs.shape[3]\n",
    "    K_mat=comm_mat(D)\n",
    "    jacobian=np.zeros((K,D**2,D**2,B))\n",
    "    \n",
    "    for k in range(K):\n",
    "        for b in range(B):\n",
    "            H=Hs[k,:,:,b]\n",
    "            L=np.copy(H)\n",
    "            for d in range(D): L[d,d]=np.exp(H[d,d])\n",
    "            \n",
    "            # get diagexp Jacobian\n",
    "            expjac=np.zeros((D**2,D**2))+np.eye(D**2)\n",
    "            for d in np.arange(0,D**2,step=D): expjac[d,d]=L[int(d/4),int(d/4)]\n",
    "            \n",
    "            # get inverse cholesky Jacobian\n",
    "            LkronI=np.kron(L,np.eye(D))\n",
    "            choljac=LkronI+np.matmul(LkronI,K_mat)\n",
    "            \n",
    "            # multiply and save result\n",
    "            jacobian[k,:,:,b]=np.matmul(choljac,expjac)\n",
    "        # end for\n",
    "    # end for\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae035e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws=np.array([[0.6,0.6],[0.4,0.4]])\n",
    "mus=np.zeros((2,2,2))\n",
    "for b in range(2): mus[:,:,b]=np.array([[2,60],[4.5,80]])\n",
    "Hs=np.zeros((2,2,2,2))\n",
    "for k in range(2):\n",
    "    for b in range(2):\n",
    "        #Hs[k,:,:,b]=np.eye(2)\n",
    "        Hs[k,:,:,b]=np.random.rand(2,2)\n",
    "        Hs[k,:,:,b]=np.tril(Hs[k,:,:,b])\n",
    "\n",
    "xc=madmix_gmm_flatten(ws,mus,Hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42153ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13116723 0.        ]\n",
      " [0.72942643 0.7360053 ]]\n"
     ]
    }
   ],
   "source": [
    "_,_,Hs_=madmix_gmm_unflatten(xc,2,2)\n",
    "print(Hs_[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "319b996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13116723, 0.        ],\n",
       "       [0.72942643, 0.7360053 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hs[0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922c8f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.29996126 0.8316617 ]\n",
      " [0.8316617  4.89005145]]\n"
     ]
    }
   ],
   "source": [
    "S=HtoSigma(Hs)\n",
    "print(S[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12fe5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13116723 0.        ]\n",
      " [0.72942643 0.7360053 ]]\n"
     ]
    }
   ],
   "source": [
    "newHs=SigmatoH(S)\n",
    "print(newHs[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae2c1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "def gen_lp(K,D):\n",
    "    \"\"\"\n",
    "    Create a log probability function for the GMM example\n",
    "    \n",
    "    Inputs:\n",
    "        K : int, number of clusters\n",
    "        D : int, dimension of data\n",
    "        \n",
    "    Outpus:\n",
    "        lp : function, log pmf of labels\n",
    "    \"\"\"\n",
    "    def lp(xd,xc,axis=None):\n",
    "        # compute the univariate log joint and conditional target pmfs\n",
    "        #\n",
    "        # inputs:\n",
    "        #    xd     : (N,B) array with labels\n",
    "        #    xc     : (K',B) array with means\n",
    "        #    axis   : int (0<axis<N), axis to find full conditional; if None then returns the log joint\n",
    "        # outputs:\n",
    "        #   ext_lprb : if axis is None, (B,) array with log joint; else, (B,K) array with d conditionals \n",
    "        N,B=xd.shape\n",
    "    \n",
    "        ws,mus,Hs=madmix_gmm_unflatten(xc,K,D)\n",
    "        Sigmas=HtoSigma(Hs)\n",
    "    \n",
    "        lprbs=np.zeros((N,K,B))\n",
    "        for k in range(K): \n",
    "            for b in range(B):\n",
    "                lprbs[:,k,b]=stats.multivariate_normal(mus[k,:,b],Sigmas[k,:,:,b]).logpdf(y)\n",
    "            # end for\n",
    "        # end for\n",
    "        lprbs=lprbs-aux.LogSumExp(np.moveaxis(lprbs,1,0))[:,np.newaxis,:]\n",
    "    \n",
    "        ext_lprb=np.zeros((N,B))\n",
    "        if axis is None: \n",
    "            ext_lprb=np.zeros((N,B))\n",
    "            for b in range(B): ext_lprb[:,b]=lprbs[np.arange(0,N),xd[:,b],b]\n",
    "            return np.sum(ext_lprb,axis=0)\n",
    "        # end if\n",
    "        return lprbs[axis,:,:].T\n",
    "    return lp\n",
    "    \n",
    "\n",
    "\n",
    "def gen_gen_grad_lp(K):\n",
    "    \"\"\"\n",
    "    Create a logp(xc) generator for the GMM example\n",
    "    \n",
    "    Inputs:\n",
    "        K : int, number of clusters\n",
    "        \n",
    "    Outpus:\n",
    "        gen_grad_lp : function, score function generator\n",
    "    \"\"\"\n",
    "    def gen_grad_lp(xd): \n",
    "        # generate the score function for Hamiltonian dynamics\n",
    "        #\n",
    "        # inputs:\n",
    "        #    xd     : (N,B) array with current labels\n",
    "        # outputs:\n",
    "        #   grad_lp : function, vectorized score function ((K',B)->(K',B))\n",
    "        #\n",
    "        # Note: K is the number of clusters, D is data dimension, \n",
    "        # and B is the number of data points (for vectorizing)\n",
    "        # K'= K (weights) + KxD (means) + KxDxD (covariances)\n",
    "    \n",
    "        idx=(xd[:,None,:]==np.arange(0,K,dtype=int)[None,:,None])               #(N,K,B)\n",
    "        N_pool=np.sum(idx,axis=0)                                               #(K,B)\n",
    "        N_pool[N_pool<1]=1                                                      #(K,B) (prevent dividing by 0)\n",
    "        y_pool=np.sum(y[:,:,None,None]*idx[:,None,:,:],axis=0)/N_pool[None,:,:] #(D,K,B)\n",
    "        diffs=y[:,:,None,None]-y_pool[None,:,:,:]                               #(N,D,K,B)\n",
    "        S_pool=np.sum(diffs[:,:,None,:,:]*diffs[:,None,:,:,:],axis=0)           #(D,D,K,B)\n",
    "        S_pool=S_pool/N_pool[None,None,:,:]                                     #(D,D,K,B)\n",
    "        S_pool=np.moveaxis(S_pool,2,0)                                          #(K,D,D,B)\n",
    "        S_poolT=np.transpose(S_pool,axes=(0,2,1,3)) # transpose DxD block, leave first and last axes untouched\n",
    "    \n",
    "        N_,D_,K_,B_= diffs.shape\n",
    "    \n",
    "        def mygrad_lp(xc): # in: (K',B)\n",
    "            # retrieve unflattened params and invert covariance matrices\n",
    "            ws,mus,Hs=madmix_gmm_unflatten(xc,K_,D_) #(K,B), (K,D,B),(K,D,D,B)\n",
    "            Sigmas=HtoSigma(Hs)\n",
    "            invSigmas=np.zeros((K_,D_,D_,B_))\n",
    "            for k in range(K_):\n",
    "                for b in range(B_):\n",
    "                    invSigmas[k,:,:,b]=np.linalg.inv(Sigmas[k,:,:,b])\n",
    "                # end for\n",
    "            # end for\n",
    "            invSigmasT=np.transpose(invSigmas,axes=(0,2,1,3)) # transpose DxD block, leave first and last axes untouched\n",
    "        \n",
    "            # more quantities\n",
    "            cluster_diffs=mus-np.moveaxis(y_pool,1,0) #(K,D,B)\n",
    "        \n",
    "            # weight score\n",
    "            grad_logw=N_pool/ws #(K,B)\n",
    "        \n",
    "            # mean score\n",
    "            grads_logmu=np.zeros((K_,D_,B_))\n",
    "            grads_logmu=-N_pool[k,None,:]*np.sum(invSigmas*cluster_diffs[:,None,:,:],axis=1) #(K,D,B)\n",
    "        \n",
    "            # cov score (wild one)\n",
    "            grads_logsigma=np.zeros((K_,D_,D_,B_))\n",
    "            grads_logsigma=-0.5*(1+N_pool[k,None,None,:])*invSigmasT #(K,D,D,B)\n",
    "            grads_logsigma-=0.5*N_pool[:,None,None,:]*cluster_diffs[:,:,None,:]*cluster_diffs[:,None,:,:] #(K,D,D,B)\n",
    "            tmpinvSigmasT=np.moveaxis(invSigmasT,3,1)\n",
    "            tmpS_poolT=np.moveaxis(S_poolT,3,1)\n",
    "            grads_logsigma+=0.5*np.moveaxis(np.matmul(tmpinvSigmasT,np.matmul(tmpS_poolT,tmpinvSigmasT)),1,3) #(K,B,D,D)->(K,D,D,B)\n",
    "        \n",
    "            # calculate jacobian\n",
    "            jac=HtoSigmaJacobian(Hs) #(K,D**2,D**2,B)\n",
    "            grads_logsigma=np.matmul(np.moveaxis(jac,3,1),grads_logsigma.reshape(K_,B_,D_**2,1,order='F')) #(K,B,D**2)\n",
    "            grads_logsigma=grads_logsigma.reshape(K_,D_,D_,B_,order='F') #(K,D,D,B)\n",
    "        \n",
    "            # add derivative wrt determinant jacobian from change of variables\n",
    "            grads_logsigma+=np.diag(D_-np.arange(D_)+1)[None,:,:,None] #(K,D,D,B)\n",
    "        \n",
    "            return madmix_gmm_flatten(grad_logw,grads_logmu,grads_logsigma) # out: (K',B)\n",
    "        return mygrad_lp\n",
    "    return gen_grad_lp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb139e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c54de72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y\u001b[38;5;241m=\u001b[39m\u001b[43mdat\u001b[49m\n\u001b[1;32m      2\u001b[0m K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m xd\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,size\u001b[38;5;241m=\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dat' is not defined"
     ]
    }
   ],
   "source": [
    "y=dat\n",
    "K=2\n",
    "xd=np.random.randint(low=0,high=2,size=(y.shape[0],2))\n",
    "ws_=np.array([[0.6,0.6],[0.4,0.4]])\n",
    "mus_=np.zeros((2,2,2))\n",
    "for b in range(2): mus_[:,:,b]=np.array([[2,60],[4.5,80]])\n",
    "sigmas_=np.zeros((2,2,2,2))\n",
    "for k in range(2):\n",
    "    for b in range(2):\n",
    "        sigmas_[k,:,:,b]=np.eye(2)\n",
    "\n",
    "xc=madmix_gmm_flatten(ws_,mus_,sigmas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(lp(xd,xc,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttgrad=gen_grad_lp(xd)\n",
    "tt=ttgrad(xc)\n",
    "madmix_gmm_unflatten(tt,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98edaa",
   "metadata": {},
   "source": [
    "## Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ca4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#  data wrangling  #\n",
    "####################\n",
    "####################\n",
    "of_dat=pd.read_table('https://gist.githubusercontent.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv')\n",
    "dat=np.array(of_dat)\n",
    "of_dat.plot.scatter('eruptions','waiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "#    Mad Mix approx    #\n",
    "########################\n",
    "########################\n",
    "\n",
    "# flow settings\n",
    "sample_size=2\n",
    "steps=100\n",
    "L=15\n",
    "epsilon=0.001\n",
    "xi=np.pi/16\n",
    "\n",
    "# initial arrays\n",
    "N,D=dat.shape\n",
    "K=2\n",
    "mu0=np.array([[2,50],[5,80]])\n",
    "sigma0=np.zeros((K,D,D))\n",
    "invsigma0=np.zeros((K,D,D))\n",
    "for k in range(K): \n",
    "    sigma0[k,:,:]=np.array([1.,10.])*np.eye(D)\n",
    "    invsigma0[k,:,:]=np.eye(D)/np.array([1.,0.1])\n",
    "# end for\n",
    "nu0=1.\n",
    "w0=np.ones(K)/K\n",
    "\n",
    "# reference sampler\n",
    "def randq0(size):\n",
    "    # discrete vars\n",
    "    rxd  = np.random.randint(low=0,high=K,size=(N,size))\n",
    "    rud  = np.random.rand(N,size)\n",
    "    \n",
    "    # continuous vars\n",
    "    Kp=K+K*D+int(K*D*(1+0.5*(D-1)))\n",
    "    rrho = np.random.laplace(size=(Kp,size))\n",
    "    ruc  = np.random.rand(size)\n",
    "    \n",
    "    # weights, means, and covs separately\n",
    "    rws=np.random.dirichlet(alpha=np.ones(K),size=size).T\n",
    "    rmus=mu0[:,:,None]+np.sum(np.random.randn(K,D,1,size)*invsigma0[:,:,:,None],axis=2)\n",
    "    rSigmas=np.zeros((K,D,D,size))\n",
    "    for k in range(K): rSigmas[k,:,:,:]=np.moveaxis(stats.invwishart(N,N*sigma0[k,:,:]).rvs(size=size),0,2)\n",
    "    rHs=SigmatoH(rSigmas)\n",
    "    rxc=madmix_gmm_flatten(rws,rmus,rHs)\n",
    "    return rxd,rud,rxc,rrho,ruc\n",
    "\n",
    "y = np.array(of_dat)\n",
    "lp = gen_lp(K,D)\n",
    "gen_grad_lp=gen_gen_grad_lp(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_,ud_,xc_,rho_,uc_=madmix.randqN(sample_size,steps,randq0,L,epsilon,lp,gen_grad_lp,xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55285f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_,mus_,Hs_=madmix_gmm_unflatten(xc_,K,D)\n",
    "Sigmas_=HtoSigma(Hs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#     results      #\n",
    "####################\n",
    "####################\n",
    "plt.scatter(dat[:,0],dat[:,1],c=np.mean(xd_,axis=-1))\n",
    "plt.plot(np.mean(mus_,axis=-1)[:,0],np.mean(mus_,axis=-1)[:,1],'*r',ms=10)\n",
    "\n",
    "xx, yy = np.mgrid[1:6:.1, 35:100:.1]\n",
    "data = np.dstack((xx, yy))\n",
    "for k in range(K):\n",
    "    rv = stats.multivariate_normal(np.mean(mus_,axis=-1)[k,:], np.mean(Sigmas_,axis=-1)[k,:,:])\n",
    "    zz = rv.pdf(data)\n",
    "    plt.contour(xx, yy, zz,levels=4,colors='grey')\n",
    "    \n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Wait time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5df00",
   "metadata": {},
   "source": [
    "## Palmer penguin data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins().dropna()\n",
    "std_penguins=(penguins-penguins.mean())/penguins.std() # normalize data\n",
    "pg_dat=np.array(std_penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=np.squeeze(np.array(penguins[['species']]))\n",
    "colors[colors=='Adelie']='#7ad151'\n",
    "colors[colors=='Gentoo']='#2a788e'\n",
    "colors[colors=='Chinstrap']='#440154'\n",
    "pd.plotting.scatter_matrix(std_penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']],c=colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c88d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "#    Mad Mix approx    #\n",
    "########################\n",
    "########################\n",
    "\n",
    "# flow settings\n",
    "sample_size=2\n",
    "steps=100\n",
    "L=15\n",
    "epsilon=0.001\n",
    "xi=np.pi/16\n",
    "\n",
    "# initial arrays\n",
    "N,D=pg_dat.shape\n",
    "K=3\n",
    "mu0=np.array([[-2.,1.,-1.,-1.],  # green\n",
    "             [1.,1.,-0.5,-0.5],  # purple\n",
    "             [1.,-1.5,1.5,2.]])  # blue\n",
    "sigma0=np.zeros((K,D,D))\n",
    "invsigma0=np.zeros((K,D,D))\n",
    "for k in range(K): \n",
    "    sigma0[k,:,:]=0.5*np.eye(D)\n",
    "    invsigma0[k,:,:]=np.eye(D)/0.5\n",
    "# end for\n",
    "w0=np.ones(K)/K\n",
    "nu0=1.\n",
    "\n",
    "# reference sampler\n",
    "def randq0(size):\n",
    "    # discrete vars\n",
    "    rxd  = np.random.randint(low=0,high=K,size=(N,size))\n",
    "    rud  = np.random.rand(N,size)\n",
    "    \n",
    "    # continuous vars\n",
    "    Kp=K+K*D+int(K*D*(1+0.5*(D-1)))\n",
    "    rrho = np.random.laplace(size=(Kp,size))\n",
    "    ruc  = np.random.rand(size)\n",
    "    \n",
    "    # weights, means, and covs separately\n",
    "    rws=np.random.dirichlet(alpha=np.ones(K),size=size).T\n",
    "    rmus=mu0[:,:,None]+np.sum(np.random.randn(K,D,1,size)*invsigma0[:,:,:,None],axis=2)\n",
    "    rSigmas=np.zeros((K,D,D,size))\n",
    "    for k in range(K): rSigmas[k,:,:,:]=np.moveaxis(stats.invwishart(N,N*sigma0[k,:,:]).rvs(size=size),0,2)\n",
    "    rHs=SigmatoH(rSigmas)\n",
    "    rxc=madmix_gmm_flatten(rws,rmus,rHs)\n",
    "    return rxd,rud,rxc,rrho,ruc\n",
    "\n",
    "y = np.array(pg_dat)\n",
    "lp = gen_lp(K,D)\n",
    "gen_grad_lp=gen_gen_grad_lp(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59987a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_,ud_,xc_,rho_,uc_=madmix.randqN(sample_size,steps,randq0,L,epsilon,lp,gen_grad_lp,xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_,mus_,Hs_=madmix_gmm_unflatten(xc_,K,D)\n",
    "Sigmas_=HtoSigma(Hs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de869c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=np.squeeze(np.array(penguins[['species']]))\n",
    "colors[np.argmax(xd_,axis=1)==0]='#440154'\n",
    "colors[np.argmax(xd_,axis=1)==1]='#7ad151'\n",
    "colors[np.argmax(xd_,axis=1)==2]='#2a788e'\n",
    "pd.plotting.scatter_matrix(penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']],c=colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3847dcf",
   "metadata": {},
   "source": [
    "## Waveform data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad40a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "waveform_dat=pd.read_table('https://hastie.su.domains/ElemStatLearn/datasets/waveform.train')\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(waveform_dat[waveform_dat.columns.difference(['row.names','y'])])\n",
    "waveform_pca=np.array(waveform_dat[waveform_dat.columns.difference(['row.names','y'])])@pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(waveform_pca[:,0],waveform_pca[:,1],c=np.squeeze(np.array(waveform_dat[['y']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_dat=waveform_pca[:,:2]\n",
    "K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ca75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "#    Mad Mix approx    #\n",
    "########################\n",
    "########################\n",
    "\n",
    "# flow settings\n",
    "sample_size=2\n",
    "steps=100\n",
    "L=15\n",
    "epsilon=0.001\n",
    "xi=np.pi/16\n",
    "\n",
    "# initial arrays\n",
    "N,D=wf_dat.shape\n",
    "K=3\n",
    "# initial arrays\n",
    "mu0=np.array([[-3.,4.],  # blue\n",
    "              [ 5.,4.],  # purple \n",
    "              [ 0.,0.]]) # yellow\n",
    "sigma0=np.zeros((K,D,D))\n",
    "invsigma0=np.zeros((K,D,D))\n",
    "for k in range(K): \n",
    "    sigma0[k,:,:]=5.*np.eye(D)\n",
    "    invsigma0[k,:,:]=np.eye(D)/5.\n",
    "w0=np.ones(K)/K\n",
    "nu0=1.\n",
    "\n",
    "# reference sampler\n",
    "def randq0(size):\n",
    "    # discrete vars\n",
    "    rxd  = np.random.randint(low=0,high=K,size=(N,size))\n",
    "    rud  = np.random.rand(N,size)\n",
    "    \n",
    "    # continuous vars\n",
    "    Kp=K+K*D+int(K*D*(1+0.5*(D-1)))\n",
    "    rrho = np.random.laplace(size=(Kp,size))\n",
    "    ruc  = np.random.rand(size)\n",
    "    \n",
    "    # weights, means, and covs separately\n",
    "    rws=np.random.dirichlet(alpha=np.ones(K),size=size).T\n",
    "    rmus=mu0[:,:,None]+np.sum(np.random.randn(K,D,1,size)*invsigma0[:,:,:,None],axis=2)\n",
    "    rSigmas=np.zeros((K,D,D,size))\n",
    "    for k in range(K): rSigmas[k,:,:,:]=np.moveaxis(stats.invwishart(N,N*sigma0[k,:,:]).rvs(size=size),0,2)\n",
    "    rHs=SigmatoH(rSigmas)\n",
    "    rxc=madmix_gmm_flatten(rws,rmus,rHs)\n",
    "    return rxd,rud,rxc,rrho,ruc\n",
    "\n",
    "y = np.array(wf_dat)\n",
    "lp = gen_lp(K,D)\n",
    "gen_grad_lp=gen_gen_grad_lp(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_,ud_,xc_,rho_,uc_=madmix.randqN(sample_size,steps,randq0,L,epsilon,lp,gen_grad_lp,xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a931b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_,mus_,Hs_=madmix_gmm_unflatten(xc_,K,D)\n",
    "Sigmas_=HtoSigma(Hs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(wf_dat[:,0],wf_dat[:,1],c=np.mean(xd_,axis=-1))\n",
    "plt.plot(np.mean(mus_,axis=-1)[:,0],np.mean(mus_,axis=-1)[:,1],'*r',ms=10)\n",
    "\n",
    "xx, yy = np.mgrid[1:6:.1, 35:100:.1]\n",
    "data = np.dstack((xx, yy))\n",
    "#for k in range(K):\n",
    "#    rv = stats.multivariate_normal(np.mean(mus_,axis=-1)[k,:], np.mean(Sigmas_,axis=-1)[k,:,:])\n",
    "#    zz = rv.pdf(data)\n",
    "#    plt.contour(xx, yy, zz,levels=4,colors='grey')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
