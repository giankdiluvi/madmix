{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a0cd04",
   "metadata": {},
   "source": [
    "# MAD Mix GMM extension\n",
    "\n",
    "To handle multivariate data and learn weights and covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c98e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sys,time\n",
    "sys.path.insert(1, '../src/')\n",
    "import madmix\n",
    "import aux\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fbcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#   auxiliary fns  #\n",
    "####################\n",
    "####################\n",
    "def madmix_gmm_flatten(ws,mus,sigmas):\n",
    "    \"\"\"\n",
    "    Flatten weights, meand, and covariances into 2D array\n",
    "    \n",
    "    Inputs:\n",
    "        ws     : (K,B) array, weights\n",
    "        mus    : (K,D,B) array, cluster means\n",
    "        sigmas : (K,D,D,B) array, cluster covariances\n",
    "    \n",
    "    Outpus:\n",
    "        xc     : (K',B) array, flattened values\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + KxDxD (covariances)\n",
    "    \"\"\"\n",
    "    K,D,B=mus.shape\n",
    "    \n",
    "    flat_mus=mus.reshape(K*D,B)\n",
    "    flat_sigmas=sigmas.reshape(K*D*D,B)\n",
    "    return np.vstack((ws,flat_mus,flat_sigmas))\n",
    "\n",
    "\n",
    "def madmix_gmm_unflatten(xc,K,D):\n",
    "    \"\"\"\n",
    "    Unflatten xc into weights, meand, and covariances\n",
    "    \n",
    "    Inputs:\n",
    "        xc     : (K',B) array, flattened values\n",
    "    \n",
    "    Outputs:\n",
    "        ws     : (K,B) array, weights\n",
    "        mus    : (K,D,B) array, cluster means\n",
    "        sigmas : (K,D,D,B) array, cluster covariances\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + KxDxD (covariances)\n",
    "    \"\"\"\n",
    "    B=xc.shape[-1]\n",
    "    \n",
    "    # recover each flattened var\n",
    "    ws=xc[:K,:]\n",
    "    flat_mus=xc[K:(K*D+K),:]\n",
    "    flat_sigmas=xc[(K*D+K):,:]\n",
    "    \n",
    "    # unflatten separately\n",
    "    mus=flat_mus.reshape(K,D,B)\n",
    "    sigmas=flat_sigmas.reshape(K,D,D,B)\n",
    "    \n",
    "    return ws,mus,sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1387f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "def lp(xd,xc,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    xd     : (N,B) array with labels\n",
    "    #    xc     : (K',B) array with means\n",
    "    #    axis   : int (0<axis<N), axis to find full conditional; if None then returns the log joint\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (B,) array with log joint; else, (B,K) array with d conditionals \n",
    "    N,B=xd.shape\n",
    "    \n",
    "    ws,mus,sigmas=madmix_gmm_unflatten(xc,K,D)\n",
    "    lprbs=np.zeros((N,K,B))\n",
    "    for k in range(K): \n",
    "        for b in range(B):\n",
    "            lprbs[:,k,b]=stats.multivariate_normal(mus[k,:,b],sigmas[k,:,:,b]).logpdf(y)\n",
    "        # end for\n",
    "    # end for\n",
    "    lprbs=lprbs-aux.LogSumExp(np.moveaxis(lprbs,1,0))[:,np.newaxis,:]\n",
    "    \n",
    "    ext_lprb=np.zeros((N,B))\n",
    "    if axis is None: \n",
    "        ext_lprb=np.zeros((N,B))\n",
    "        for b in range(B): ext_lprb[:,b]=lprbs[np.arange(0,N),xd[:,b],b]\n",
    "        return np.sum(ext_lprb,axis=0)\n",
    "    # end if\n",
    "    return lprbs[axis,:,:].T\n",
    "\n",
    "\n",
    "\n",
    "def gen_grad_lp(xd): \n",
    "    # generate the score function for Hamiltonian dynamics\n",
    "    #\n",
    "    # inputs:\n",
    "    #    xd     : (N,B) array with current labels\n",
    "    # outputs:\n",
    "    #   grad_lp : function, vectorized score function ((K',B)->(K',B))\n",
    "    #\n",
    "    # Note: K is the number of clusters, D is data dimension, \n",
    "    # and B is the number of data points (for vectorizing)\n",
    "    # K'= K (weights) + KxD (means) + KxDxD (covariances)\n",
    "    \n",
    "    idx=(xd==np.arange(0,K,dtype=int)[None,:,None])                         #(N,K,B)\n",
    "    N_pool=np.sum(idx,axis=0)                                               #(K,B)\n",
    "    y_pool=np.sum(y[:,:,None,None]*idx[:,None,:,:],axis=0)/N_pool[None,:,:] #(D,K,B)\n",
    "    diffs=y[:,:,None,None]-y_pool[None,:,:,:]                               #(N,D,K,B)\n",
    "    S_pool=np.sum(diffs[:,:,None,:,:]*diffs[:,None,:,:,:],axis=0)           #(D,D,K,B)\n",
    "    S_pool=S_pool/N_pool[None,None,:,:]                                     #(D,D,K,B)\n",
    "    S_pool=np.moveaxis(S_pool,2,0)                                          #(K,D,D,B)\n",
    "    S_poolT=np.transpose(S_pool,axes=(0,2,1,3)) # transpose DxD block, leave first and last axes untouched\n",
    "    \n",
    "    N_,D_,K_,B_= diffs.shape\n",
    "    \n",
    "    def mygrad_lp(xc): # in: (K',B)\n",
    "        # retrieve unflattened params and invert covariance matrices\n",
    "        ws,mus,Sigmas=madmix_gmm_unflatten(xc,K_,D_) #(K,B), (K,D,B),(K,D,D,B)\n",
    "        invSigmas=np.zeros((K,D,D,B))\n",
    "        for k in range(K):\n",
    "            for b in range(B):\n",
    "                invSigmas[k,:,:,b]=np.linalg.inv(Sigmas[k,:,:,b])\n",
    "            # end for\n",
    "        # end for\n",
    "        invSigmasT=np.transpose(invSigmas,axes=(0,2,1,3)) # transpose DxD block, leave first and last axes untouched\n",
    "        \n",
    "        # more quantities\n",
    "        cluster_diffs=np.moveaxis(y_pool,1,0)-mus #(K,D,B)\n",
    "        \n",
    "        # calculate separate gradients\n",
    "        grad_logw=N_pool/ws #(K,B)\n",
    "        grads_logmu=np.zeros((K,D,B))\n",
    "        grads_logsigma=np.zeros((K,D,D,B))\n",
    "        for k in range(K):\n",
    "            grads_logmu[k,:,:]=-N_pool[None,k,:]*np.sum(invSigmas[k,:,:,:]*cluster_diffs[k,None,:,:],axis=0) #(D,B)\n",
    "            grads_logsigma[k,:,:,:]=-0.5*(1+N_pool[k,None,None,:])*invSigmasT[k,:,:,:] #(D,D,B)\n",
    "            grads_logsigma[k,:,:,:]-=0.5*N_pool[k,None,None:]*cluster_diffs[k,:,None,:]*cluster_diffs[k,None,:,:] #(D,D,B)\n",
    "            grads_logsigma[k,:,:,:]+=0.5*np.matmul(invSigmasT[k,:,:,:],np.matmul(S_poolT[k,:,:,:],invSigmasT[k,:,:,:])) #(D,D,B)\n",
    "        # end for\n",
    "        \n",
    "        #grads_logmu=-N_pool[k,None,:]*np.sum(invSigmas*cluster_diffs[:,None,:,:],axis=1) #(K,D,B)\n",
    "        #grads_logsigma=-0.5*(1+N_pool[k,None,None,:])*invSigmasT #(K,D,D,B)\n",
    "        #grads_logsigma-=0.5*N_pool[:,None,None:]*cluster_diffs[:,:,None,:]*cluster_diffs[:,None,:,:] #(K,D,D,B)\n",
    "        #grads_logsigma+=0.5*np.matmul(invSigmasT,np.matmul(S_poolT,invSigmasT)) #(K,D,D,B)\n",
    "        \n",
    "        return madmix_gmm_flatten(grad_logw,grads_logmu,grads_logsigma) # out: (K',B)\n",
    "    return mygrad_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0735d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dat\n",
    "K=2\n",
    "xd=np.random.randint(low=0,high=2,size=(y.shape[0],2))\n",
    "ws_=np.array([[0.6,0.6],[0.4,0.4]])\n",
    "mus_=np.zeros((2,2,2))\n",
    "for b in range(2): mus_[:,:,b]=np.array([[2,60],[4.5,80]])\n",
    "sigmas_=np.zeros((2,2,2,2))\n",
    "for k in range(2):\n",
    "    for b in range(2):\n",
    "        sigmas_[k,:,:,b]=np.eye(2)\n",
    "\n",
    "xc=madmix_gmm_flatten(ws_,mus_,sigmas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b08408ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.79888842e-79, 1.00000000e+00],\n",
       "       [2.79888842e-79, 1.00000000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lp(xd,xc,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c907b7a",
   "metadata": {},
   "source": [
    "## Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbef89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#  data wrangling  #\n",
    "####################\n",
    "####################\n",
    "of_dat=pd.read_table('https://gist.githubusercontent.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv')\n",
    "dat=np.array(of_dat)\n",
    "N,D=dat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
