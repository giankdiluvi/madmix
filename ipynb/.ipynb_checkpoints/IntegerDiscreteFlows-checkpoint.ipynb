{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc052cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, time\n",
    "sys.path.insert(1, '../discrete_mixflows/')\n",
    "from discrete_mixflows import *\n",
    "from gibbs import *\n",
    "from concrete import *\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8553f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "np.random.seed(2023)\n",
    "K1=10\n",
    "prbs=np.random.rand(K1)\n",
    "prbs=prbs/np.sum(prbs)\n",
    "def lp(x,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (1,d) array with state values\n",
    "    #    axis : int, full conditional to calculate; returns joint if None\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (d,) array with log joint; else, (d,K1) array with d conditionals \n",
    "    \n",
    "    ext_lprb=np.log(np.repeat(prbs[:,np.newaxis],x.shape[1],axis=1).T)\n",
    "    if axis==None: return np.squeeze(ext_lprb[np.arange(0,x.shape[1]),x])\n",
    "    return ext_lprb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308d16a",
   "metadata": {},
   "source": [
    "## IDF\n",
    "from https://github.com/jmtomczak/intro_dgm/blob/main/flows/idf_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4929f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chakraborty & Chakravarty, \"A new discrete probability distribution with integer support on (−∞, ∞)\",\n",
    "#  Communications in Statistics - Theory and Methods, 45:2, 492-505, DOI: 10.1080/03610926.2013.830743\n",
    "\n",
    "def log_min_exp(a, b, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "    Computes the log of exp(a) - exp(b) in a (more) numerically stable fashion.\n",
    "    Using:\n",
    "     log(exp(a) - exp(b))\n",
    "     c + log(exp(a-c) - exp(b-c))\n",
    "     a + log(1 - exp(b-a))\n",
    "    And note that we assume b < a always.\n",
    "    \"\"\"\n",
    "    y = a + torch.log(1 - torch.exp(b - a) + epsilon)\n",
    "\n",
    "    return y\n",
    "\n",
    "def log_integer_probability(x, mean, logscale):\n",
    "    scale = torch.exp(logscale)\n",
    "\n",
    "    logp = log_min_exp(\n",
    "        F.logsigmoid((x + 0.5 - mean) / scale),\n",
    "        F.logsigmoid((x - 0.5 - mean) / scale))\n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1082b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        rounded = torch.round(input, out=None)\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736632d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D=2):\n",
    "        super(IDF, self).__init__()\n",
    "\n",
    "        print('IDF by JT.')\n",
    "        \n",
    "        if len(netts) == 1:\n",
    "            self.t = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "        \n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = torch.nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = torch.nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = torch.nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('You can provide either 1 or 4 translation nets.')\n",
    "        \n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        self.round = RoundStraightThrough.apply\n",
    "        \n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        \n",
    "        if self.idf_git == 1:\n",
    "            (xa, xb) = torch.chunk(x, 2, 1)\n",
    "            \n",
    "            if forward:\n",
    "                yb = xb + self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.round(self.t[index](xa))\n",
    "            \n",
    "            return torch.cat((xa, yb), 1)\n",
    "        \n",
    "        elif self.idf_git == 4:\n",
    "            (xa, xb, xc, xd) = torch.chunk(x, 4, 1)\n",
    "            \n",
    "            if forward:\n",
    "                ya = xa + self.round(self.t_a[index](torch.cat((xb, xc, xd), 1)))\n",
    "                yb = xb + self.round(self.t_b[index](torch.cat((ya, xc, xd), 1)))\n",
    "                yc = xc + self.round(self.t_c[index](torch.cat((ya, yb, xd), 1)))\n",
    "                yd = xd + self.round(self.t_d[index](torch.cat((ya, yb, yc), 1)))\n",
    "            else:\n",
    "                yd = xd - self.round(self.t_d[index](torch.cat((xa, xb, xc), 1)))\n",
    "                yc = xc - self.round(self.t_c[index](torch.cat((xa, xb, yd), 1)))\n",
    "                yb = xb - self.round(self.t_b[index](torch.cat((xa, yc, yd), 1)))\n",
    "                ya = xa - self.round(self.t_a[index](torch.cat((yb, yc, yd), 1)))\n",
    "            \n",
    "            return torch.cat((ya, yb, yc, yd), 1)\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -self.log_prior(z).sum()\n",
    "        else:\n",
    "            return -self.log_prior(z).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        # sample z:\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D)\n",
    "        # x = f^-1(z)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        # Sample from logistic\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        # And then round it to an integer.\n",
    "        return torch.round(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            \n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = prbs.shape[0]   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77facd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e16b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIDF(depth,lprbs,layers=32):\n",
    "    dim=lprbs.shape[0]\n",
    "\n",
    "    nett = lambda: nn.Sequential(nn.Linear(dim // 2, layers), nn.LeakyReLU(),\n",
    "                                     nn.Linear(layers, layers), nn.LeakyReLU(),\n",
    "                                     nn.Linear(layers, dim // 2))\n",
    "    netts = [nett]\n",
    "\n",
    "    # Init IDF\n",
    "    return IDF(netts, depth, D=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16d88e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIDF(depth,lprbs,layers=32,max_iters=1000,lr=1e-4,mc_ss=1000,seed=0,verbose=True):\n",
    "    \"\"\"\n",
    "    Train a RealNVP normalizing flow targeting lprbs using the Adam optimizer\n",
    "    \n",
    "    Input:\n",
    "        temp      : float, temperature of Concrete relaxation\n",
    "        depth     : int, number of couplings (transformations)\n",
    "        lprbs     : (dim,) array, target log probabilities\n",
    "        layers    : int, width of the linear layers\n",
    "        max_iters : int, max number of Adam iters\n",
    "        lr        : float, Adam learning rate\n",
    "        mc_ss     : int, number of samples to draw from target for training\n",
    "        seed      : int, for reproducinility\n",
    "        verbose   : boolean, indicating whether to print loss every 100 iterations of Adam\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # create flow\n",
    "    flow=createIDF(depth,lprbs,layers)\n",
    "    target = torch.distributions.categorical.Categorical(torch.tensor(np.exp(lprbs)))\n",
    "    \n",
    "    # train flow\n",
    "    sample=target.sample((mc_ss,)).float()\n",
    "    optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=lr)\n",
    "    losses=np.zeros(max_iters)\n",
    "    \n",
    "    for t in range(max_iters):\n",
    "        loss = -flow.forward(sample).mean()\n",
    "        losses[t]=loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and t%(max_iters//10) == 0: print('iter %s:' % t, 'loss = %.3f' % loss)\n",
    "    # end for\n",
    "    return flow,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc7beac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF by JT.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flow,losses\u001b[38;5;241m=\u001b[39m\u001b[43mtrainIDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlprbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmc_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [25], line 28\u001b[0m, in \u001b[0;36mtrainIDF\u001b[0;34m(depth, lprbs, layers, max_iters, lr, mc_ss, seed, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m losses\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(max_iters)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     29\u001b[0m     losses[t]\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn [6], line 78\u001b[0m, in \u001b[0;36mIDF.forward\u001b[0;34m(self, x, reduction)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior(z)\u001b[38;5;241m.\u001b[39msum()\n",
      "Cell \u001b[0;32mIn [6], line 64\u001b[0m, in \u001b[0;36mIDF.f\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m z \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_flows):\n\u001b[0;32m---> 64\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoupling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute(z)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "Cell \u001b[0;32mIn [6], line 33\u001b[0m, in \u001b[0;36mIDF.coupling\u001b[0;34m(self, x, index, forward)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoupling\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, index, forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_git \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m         (xa, xb) \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m forward:\n\u001b[1;32m     36\u001b[0m             yb \u001b[38;5;241m=\u001b[39m xb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt[index](xa))\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "flow,losses=trainIDF(depth=10,lprbs=np.log(prbs),layers=32,max_iters=1000,lr=1e-4,mc_ss=1000,seed=2023,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
