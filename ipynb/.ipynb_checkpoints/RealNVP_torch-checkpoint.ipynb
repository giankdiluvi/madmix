{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6319c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, '../discrete_mixflows/')\n",
    "from discrete_mixflows import *\n",
    "from gibbs import *\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb35056",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "np.random.seed(2023)\n",
    "K1=10\n",
    "prbs=np.random.rand(K1)\n",
    "prbs=prbs/np.sum(prbs)\n",
    "def lp(x,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (1,d) array with state values\n",
    "    #    axis : int, full conditional to calculate; returns joint if None\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (d,) array with log joint; else, (d,K1) array with d conditionals \n",
    "    \n",
    "    ext_lprb=np.log(np.repeat(prbs[:,np.newaxis],x.shape[1],axis=1).T)\n",
    "    if axis==None: return np.squeeze(ext_lprb[np.arange(0,x.shape[1]),x])\n",
    "    return ext_lprb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35eec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import flowtorch.bijectors as bij\n",
    "import flowtorch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowtorch.parameters as params\n",
    "# Lazily instantiated flow plus base and target distributions\n",
    "params = params.DenseAutoregressive(hidden_dims=(32,))\n",
    "bijectors = bij.AffineAutoregressive(params)#params=params)\n",
    "base_dist = torch.distributions.Independent(torch.distributions.RelaxedOneHotCategorical(\n",
    "      torch.tensor([temperature]),\n",
    "      torch.tensor(np.ones(K1)/K1)),0)\n",
    "target_dist = torch.distributions.Independent(torch.distributions.RelaxedOneHotCategorical(\n",
    "      torch.tensor([temperature]),\n",
    "      torch.tensor(prbs)),0)\n",
    "\n",
    "# Instantiate transformed distribution and parameters\n",
    "flow = dist.Flow(base_dist, bijectors)\n",
    "\n",
    "# Training loop\n",
    "opt = torch.optim.Adam(flow.parameters(), lr=5e-3)\n",
    "frame = 0\n",
    "for idx in range(3001):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Minimize KL(q || p)\n",
    "    y = base_dist.sample((1000,)).to(torch.float32)\n",
    "    print(flow.bijector.inverse(y))\n",
    "    loss = -flow.log_prob(y).mean()\n",
    "\n",
    "    if idx % 500 == 0:\n",
    "        print('epoch', idx, 'loss', loss)\n",
    "        \n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
