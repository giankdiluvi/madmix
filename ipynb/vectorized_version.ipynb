{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee985e06",
   "metadata": {},
   "source": [
    "# Deterministic slice flows\n",
    "\n",
    "Vectorized version of the functions in `density_evaluation`.\n",
    "\n",
    "Consider a target density $\\pi(x_{1:M},u_{1:M})=p(x_{1:M})1_{[0,1]}(u_{1:M})$ \n",
    "where we have access to the full conditionals $p_m$ \n",
    "and let $F_m,Q_m$ be the cdf and quantile functions of $p_m$.\n",
    "We approximate $\\pi$ with a variational ergodic flow\n",
    "$$\n",
    "    q_N=\\frac{1}{N}\\sum_{n=0}^N T^nq_0,\n",
    "$$\n",
    "where $q_0$ is a reference distribution and\n",
    "$T=T_m\\circ\\cdots\\circ T_1$ mimics Gibbs sampling, i.e.,\n",
    "each map $T_m$ updates only $(x_m,u_m)\\mapsto(x_m',u_m')$.\n",
    "Specifically,\n",
    "$$\n",
    "\\begin{pmatrix}x_m'\\\\u_m'\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "    Q_m(\\rho(x_m,u_m)+\\xi\\mod 1)\\\\ \n",
    "    \\frac{1}{p_m(x_m')}((\\rho(x_m,u_m)+\\xi\\mod 1)-F(x_m'))\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where $\\rho(x,u)=F_m(x-1)+up_m(x)$ converts to proportions and $F_m(0)=0$ by convention.\n",
    "\n",
    "In the notes,\n",
    "I showed that the variational density can be evaluated in closed form\n",
    "since the Jacobians of the continuous restriction correspond to density ratios. Specifically,\n",
    "$$\n",
    "    q_N(x_{1:M},u_{1:M})\n",
    "    =\\frac{1}{N}\\sum_{n=0}^{N-1}\n",
    "    q_0(T^{-n}(x_{1:M},u_{1:M})\n",
    "    \\prod_{j=1}^n\\prod_{m=1}^M \\frac{p_m(T^{-j+1}(x_{1:M},u_{1:M}))}{p_m(T^{-j}(x_{1:M},u_{1:M}))}.\n",
    "$$\n",
    "\n",
    "First we define all the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338689ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933bdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "# variational approximation functions\n",
    "########################################\n",
    "########################################\n",
    "def lqN(x,u,N,lq0,lp,xi=np.pi/16):\n",
    "    # evaluate variational log likelihood log qN(x,u)\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x         : (M,d) array, states of xm\n",
    "    #    u         : (M,d) array, values of um\n",
    "    #    N         : int, variational parameter; max number of applications of T\n",
    "    #    lq0       : function, reference log likelihood (vectorized)\n",
    "    #    lp        : function, target log likelihood (vectorized)\n",
    "    #    xi        : scalar, uniform shift\n",
    "    #\n",
    "    # outputs:\n",
    "    #   logqN(x,u) : (d,) array, likelihood at each datum x_i\n",
    "    \n",
    "    if N==1: return lq0(x,u)\n",
    "    w=np.zeros((N,x.shape[1]))\n",
    "    w[0,:]=lq0(x,u)\n",
    "    LJ=np.zeros(x.shape[1])\n",
    "    for n in range(N-1):\n",
    "        x,u,tlj=flow(x,u,1,lp,xi,direction='bwd')\n",
    "        LJ=LJ+tlj\n",
    "        w[n+1,:]=lq0(x,u)+LJ\n",
    "    # end for\n",
    "    return LogSumExp(w)-np.log(N)\n",
    "\n",
    "def randqN(size,N,lp,randq0,xi=np.pi/16):\n",
    "    # generate samples from the variational distribution qN\n",
    "    #\n",
    "    # inputs:\n",
    "    #    size   : int, number of samples to generate\n",
    "    #    N      : int, variational parameter; max number of applications of T\n",
    "    #    lp        : function, target log likelihood (vectorized)\n",
    "    #    randq0 : function, reference distribution sampler\n",
    "    #    xi        : scalar, uniform shift\n",
    "    #\n",
    "    # outputs:\n",
    "    #   sx      : (M,size) array, x samples from qN\n",
    "    #   su      : (M,size,) array, u samples from qN\n",
    "    \n",
    "    if N==1: return randq0(size)\n",
    "    K=np.random.randint(low=0,high=N,size=size)\n",
    "    x,u=randq0(size)\n",
    "    for n in range(N-1):\n",
    "        tx,tu,_ = flow(x[:,K>=n+1],u[:,K>=n+1],steps=1,lp=lp,xi=xi,direction='fwd') # update those with large enough K\n",
    "        x[:,K>=n+1]=tx \n",
    "        u[:,K>=n+1]=tu\n",
    "    return x,u\n",
    "    \n",
    "\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "# flow functions\n",
    "########################################\n",
    "########################################\n",
    "def flow(x,u,steps,lp,xi=np.pi/16,direction='fwd'):\n",
    "    # compute T^n(x,u)\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x         : (M,d) array, initial states of x\n",
    "    #    u         : (M,d) array, initial values of u\n",
    "    #    steps     : int, number of applications of T, n\n",
    "    #    lp        : function, posterior and conditional pmf\n",
    "    #    xi        : scalar, uniform shift\n",
    "    #    direction : string, one of 'fwd' (forward map) or 'bwd' (backward map)\n",
    "    #\n",
    "    # outputs:\n",
    "    #   x' : (M,d) array, updated states x'\n",
    "    #   u' : (M,d) array, updated values u'\n",
    "    \n",
    "    M=x.shape[0]\n",
    "    ljs=np.zeros(x.shape)\n",
    "    if steps==0: return x,u\n",
    "    for t in range(steps):\n",
    "        for m in range(M):\n",
    "            m_idx = m if direction=='fwd'else M-m-1 # if in reverse, update starting from the end\n",
    "            tmp_prbs=np.atleast_1d(np.exp(lp(x,axis=m_idx)))\n",
    "            tmp_prbs=tmp_prbs/np.sum(tmp_prbs,axis=1)[:,np.newaxis]\n",
    "            tx,tu,tlj=Tm(x[m_idx,:],u[m_idx,:],tmp_prbs,xi,direction=direction)\n",
    "            x[m_idx,:]=tx\n",
    "            u[m_idx,:]=tu\n",
    "            ljs[m_idx,:]=tlj\n",
    "        # end for\n",
    "    # end for\n",
    "    return x,u,np.sum(ljs,axis=0)\n",
    "        \n",
    "    \n",
    "def Tm(x,u,prbs,xi=np.pi/16,direction='fwd'):\n",
    "    # compute Tm(x,u)\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x         : (d,) array, states of xm\n",
    "    #    u         : (d,) array, values of um\n",
    "    #    prbs      : (d,Km) array, probabilities of Xm|X-m for each of the d xm's\n",
    "    #    xi        : scalar, uniform shift\n",
    "    #    direction : string, one of 'fwd' (forward map) or 'bwd' (backward map)\n",
    "    #\n",
    "    # outputs:\n",
    "    #   xp : (d,) array, updated states xm'\n",
    "    #   up : (d,) array, updated values um'\n",
    "    \n",
    "    if direction=='bwd': xi=-xi\n",
    "    p=getp(x,u,prbs,xi)\n",
    "    xp=quantile(p,prbs)\n",
    "    up=(p-cdf(xp-1,prbs))/prbs[np.arange(0,xp.shape[0]),xp]\n",
    "    return xp,up,np.log(prbs[np.arange(0,x.shape[0]),x])-np.log(prbs[np.arange(0,xp.shape[0]),xp])\n",
    "\n",
    "\n",
    "def getp(x,u,prbs,xi=np.pi/16):\n",
    "    # get proportion from current pair (xm,um)\n",
    "    # equivalent to rho+xi mod 1 in paper\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x         : (d,) array, states of xm\n",
    "    #    u         : (d,) array, values of um\n",
    "    #    prbs      : (d,Km) array, probabilities of Xm|X-m for each of the d xm's\n",
    "    #    xi        : scalar, uniform shift\n",
    "    #\n",
    "    # outputs:\n",
    "    #   p' : (d,) array, proportion and shifted states p'\n",
    "    \n",
    "    p=u*prbs[np.arange(0,x.shape[0]),x]\n",
    "    F=np.cumsum(prbs,axis=1) # cdf\n",
    "    p[x>0]=p[x>0]+np.squeeze(F[np.where(x>0),x[x>0]-1]) # vectorized \"+prbs[:x] if x>0\"\n",
    "    return (p+xi)%1\n",
    "    \n",
    "    \n",
    "########################################\n",
    "########################################\n",
    "# inference\n",
    "########################################\n",
    "########################################\n",
    "def elbo(size,N,lp,lq0,randqN,randq0,xi=np.pi/16):\n",
    "    # estimate ELBO(qN||p)\n",
    "    #\n",
    "    # inputs:\n",
    "    #    size   : int, number of samples to generate for MC estimation\n",
    "    #    N      : int, variational parameter; max number of applications of T\n",
    "    #    lp     : function, target log likelihood (vectorized)\n",
    "    #    lq0    : function, reference log likelihood\n",
    "    #    randqN : function, variational distribution sampler\n",
    "    #    randq0 : function, reference distribution sampler\n",
    "    #    xi     : scalar, uniform shift\n",
    "    #\n",
    "    # outputs:\n",
    "    #   elbo    : scalar, estimate of the ELBO\n",
    "    \n",
    "    tx,tu=randqN(size,N,lp,randq0)\n",
    "    return np.mean(lp(tx)-lqN(tx,tu,N,lq0,lp,xi))\n",
    "        \n",
    "\n",
    "########################################\n",
    "########################################\n",
    "# auxiliary functions\n",
    "########################################\n",
    "########################################\n",
    "def LogSumExp(w):\n",
    "    # LogSumExp trick\n",
    "    #\n",
    "    # inputs:\n",
    "    #    w : (N,d) array, exponents\n",
    "    #\n",
    "    # outputs:\n",
    "    #    w' : (N,d) array, log(sum(exp(w)))\n",
    "    wmax = np.amax(w,axis=0)\n",
    "    return wmax + np.log(np.sum(np.exp(w-wmax[np.newaxis,:]),axis=0))\n",
    "\n",
    "def cdf(x,prbs): \n",
    "    # cdf of x given prbs (vectorized): F(x)\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (d,) array, states of xm\n",
    "    #    prbs : (d,Km) array, probabilities of Xm|X-m for each of the d xm's\n",
    "    #\n",
    "    # outputs:\n",
    "    #   F(x) : (d,) array, cdf of X at each xi (F(x)_i=F(x_i))\n",
    "    \n",
    "    F=np.hstack((np.zeros((prbs.shape[0],1)),np.cumsum(prbs,axis=1))) # adding 0's so F(0)=0\n",
    "    return F[np.arange(0,x.shape[0]),x+1]\n",
    "\n",
    "def quantile(u,prbs): \n",
    "    # quantile function of u given prbs (badly vectorized)\n",
    "    # via scipy stats, couldn't implement in native numpy\n",
    "    #\n",
    "    # inputs:\n",
    "    #    u    : (d,) array, values of um\n",
    "    #    prbs : (d,Km) array, probabilities of Xm|X-m for each of the d xm's\n",
    "    #\n",
    "    # outputs:\n",
    "    #   Q(x) : (d,) array, quantile of X at each ui (Q(u)_i=Q(u_i))\n",
    "    quants=np.zeros(u.shape[0])\n",
    "    for d in range(u.shape[0]):\n",
    "        tmprv=stats.rv_discrete(values=(np.arange(0,prbs.shape[1]), prbs[d,:]))\n",
    "        quants[d]=tmprv.ppf(u[d])\n",
    "    return quants.astype(int)\n",
    "\n",
    "\n",
    "def gen_lp(prbs):\n",
    "    # generate an iterable lp function given probs array prbs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    prbs : (K1,...,KM) array, probabilities\n",
    "    #\n",
    "    # outputs:\n",
    "    #   my_lp : function, obtains joint and conditional probabilities\n",
    "    #           my_lp(x)      -> joint at states x\n",
    "    #           my_lp(x,axis) -> conditional of x_axis given x_{-axis}\n",
    "    \n",
    "    def my_lp(x,axis=None):\n",
    "        if axis==None: return prbs[tuple(x)] # evaluate lp(x)\n",
    "        # else return prbs[x_1,x_2,...,x_{m-1},:,x_{m+1},...,x_M] with m=axis\n",
    "        tmp_prbs=np.ones(prbs.shape[axis]) # init uniform\n",
    "        tmp_x=np.copy(x)\n",
    "        for i in range(prbs.shape[axis]):\n",
    "            tmp_x[axis]=i\n",
    "            tmp_prbs[i]=prbs[tuple(tmp_x)] \n",
    "        # end for\n",
    "        return tmp_prbs\n",
    "    return my_lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096b853",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22a6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the distribution\n",
    "np.random.seed(2022)\n",
    "K1=4\n",
    "prbs=np.random.rand(K1)\n",
    "prbs=prbs/np.sum(prbs)\n",
    "def lp(x,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (M,d) array with state values\n",
    "    #    axis : int, variable to condition on; returns joint if None\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (d,) array with log joint; else, (d,K1) array with d conditionals \n",
    "    \n",
    "    ext_lprb=np.log(np.repeat(prbs[:,np.newaxis],x.shape[1],axis=1).T)\n",
    "    if axis==None: return np.squeeze(ext_lprb[np.arange(0,x.shape[1]),x])\n",
    "    return ext_lprb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f6fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lq0 = lambda x,u : np.log(1/K1)*np.ones(x.shape[1])\n",
    "def randq0(size): return np.random.randint(0,K1,size).reshape(1,size),np.random.rand(1,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6763476",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "size=1000\n",
    "N=100\n",
    "\n",
    "# evaluate density and generate samples\n",
    "x=np.array([[0,1,2,3]],dtype=int)\n",
    "u=0.5*np.ones((1,K1))\n",
    "qvar=np.exp(lqN(x,u,N,lq0,lp))\n",
    "\n",
    "xsamples,_=randqN(size,N,lp,randq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25165d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttx=np.arange(0,prbs.shape[0])\n",
    "plt.bar(ttx+0.5,prbs, alpha=0.5, label=r'$p(x)$',color=\"black\")\n",
    "plt.plot(ttx+0.5,qvar/np.sum(qvar),c=\"black\", label=r'$q_N(x)$')\n",
    "plt.hist(xsamples[0,:],density=True,bins=[0,1,2,3,4],alpha=0.35, label='N='+str(N), color=\"#FCFFA4FF\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel('prob')\n",
    "plt.title(r'veracity of approximation for large $N$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099db3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "sims=10\n",
    "size=100\n",
    "elbos =np.zeros(sims)\n",
    "for i in range(sims): \n",
    "    print(str(i+1)+'/'+str(sims),end='\\r')\n",
    "    elbos[i]=elbo(size=size,N=2+10*i,lp=lp,lq0=lq0,randqN=randqN,randq0=randq0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a42ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,11),elbos)\n",
    "plt.xlabel(r'$N$')\n",
    "plt.xticks(ticks=[2,4,6,8,10],labels=[22,42,62,82,102])\n",
    "plt.ylabel(r'ELBO($q_N,p$)')\n",
    "plt.title(r'ELBO of approximation for different $N$')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
