{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e65bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, time\n",
    "sys.path.insert(1, '../discrete_mixflows/')\n",
    "from discrete_mixflows import *\n",
    "from gibbs import *\n",
    "from concrete import *\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39bd23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "np.random.seed(2023)\n",
    "K1=10\n",
    "prbs=np.random.rand(K1)\n",
    "prbs=prbs/np.sum(prbs)\n",
    "def lp(x,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (1,d) array with state values\n",
    "    #    axis : int, full conditional to calculate; returns joint if None\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (d,) array with log joint; else, (d,K1) array with d conditionals \n",
    "    \n",
    "    ext_lprb=np.log(np.repeat(prbs[:,np.newaxis],x.shape[1],axis=1).T)\n",
    "    if axis==None: return np.squeeze(ext_lprb[np.arange(0,x.shape[1]),x])\n",
    "    return ext_lprb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b57f5",
   "metadata": {},
   "source": [
    "## IDF\n",
    "from https://github.com/jmtomczak/intro_dgm/blob/main/flows/idf_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191de313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chakraborty & Chakravarty, \"A new discrete probability distribution with integer support on (−∞, ∞)\",\n",
    "#  Communications in Statistics - Theory and Methods, 45:2, 492-505, DOI: 10.1080/03610926.2013.830743\n",
    "\n",
    "def log_min_exp(a, b, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "    Computes the log of exp(a) - exp(b) in a (more) numerically stable fashion.\n",
    "    Using:\n",
    "     log(exp(a) - exp(b))\n",
    "     c + log(exp(a-c) - exp(b-c))\n",
    "     a + log(1 - exp(b-a))\n",
    "    And note that we assume b < a always.\n",
    "    \"\"\"\n",
    "    y = a + torch.log(1 - torch.exp(b - a) + epsilon)\n",
    "\n",
    "    return y\n",
    "\n",
    "def log_integer_probability(x, mean, logscale):\n",
    "    scale = torch.exp(logscale)\n",
    "\n",
    "    logp = log_min_exp(\n",
    "        F.logsigmoid((x + 0.5 - mean) / scale),\n",
    "        F.logsigmoid((x - 0.5 - mean) / scale))\n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df0db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        rounded = torch.round(input, out=None)\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710abeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D=2):\n",
    "        super(IDF, self).__init__()\n",
    "\n",
    "        print('IDF by JT.')\n",
    "        \n",
    "        if len(netts) == 1:\n",
    "            self.t = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "        \n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = torch.nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = torch.nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = torch.nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('You can provide either 1 or 4 translation nets.')\n",
    "        \n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        self.round = RoundStraightThrough.apply\n",
    "        \n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        \n",
    "        if self.idf_git == 1:\n",
    "            (xa, xb) = torch.chunk(x, 2, 1)\n",
    "            \n",
    "            if forward:\n",
    "                yb = xb + self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.round(self.t[index](xa))\n",
    "            \n",
    "            return torch.cat((xa, yb), 1)\n",
    "        \n",
    "        elif self.idf_git == 4:\n",
    "            (xa, xb, xc, xd) = torch.chunk(x, 4, 1)\n",
    "            \n",
    "            if forward:\n",
    "                ya = xa + self.round(self.t_a[index](torch.cat((xb, xc, xd), 1)))\n",
    "                yb = xb + self.round(self.t_b[index](torch.cat((ya, xc, xd), 1)))\n",
    "                yc = xc + self.round(self.t_c[index](torch.cat((ya, yb, xd), 1)))\n",
    "                yd = xd + self.round(self.t_d[index](torch.cat((ya, yb, yc), 1)))\n",
    "            else:\n",
    "                yd = xd - self.round(self.t_d[index](torch.cat((xa, xb, xc), 1)))\n",
    "                yc = xc - self.round(self.t_c[index](torch.cat((xa, xb, yd), 1)))\n",
    "                yb = xb - self.round(self.t_b[index](torch.cat((xa, yc, yd), 1)))\n",
    "                ya = xa - self.round(self.t_a[index](torch.cat((yb, yc, yd), 1)))\n",
    "            \n",
    "            return torch.cat((ya, yb, yc, yd), 1)\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -self.log_prior(z).sum()\n",
    "        else:\n",
    "            return -self.log_prior(z).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        # sample z:\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D)\n",
    "        # x = f^-1(z)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        # Sample from logistic\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        # And then round it to an integer.\n",
    "        return torch.round(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62210caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            \n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3023d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF by JT.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The number of invertible transformations\n",
    "num_flows = 8\n",
    "\n",
    "# This variable defines whether we use: \n",
    "#   1 - the classic coupling layer proposed in (Hogeboom et al., 2019)\n",
    "#   4 - the general invertible transformation in (Tomczak, 2020) with 4 partitions\n",
    "idf_git = 1\n",
    "\n",
    "if idf_git == 1:\n",
    "    nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                                     nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                                     nn.Linear(M, D // 2))\n",
    "    netts = [nett]\n",
    "\n",
    "elif idf_git == 4:\n",
    "    nett_a = lambda: nn.Sequential(nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, D // 4))\n",
    "\n",
    "    nett_b = lambda: nn.Sequential(nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, D // 4))\n",
    "\n",
    "    nett_c = lambda: nn.Sequential(nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, D // 4))\n",
    "\n",
    "    nett_d = lambda: nn.Sequential(nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                                       nn.Linear(M, D // 4))\n",
    "    \n",
    "    netts = [nett_a, nett_b, nett_c, nett_d]\n",
    "\n",
    "# Init IDF\n",
    "model = IDF(netts, num_flows, D=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e86fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97bac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
