{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd3c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, time\n",
    "sys.path.insert(1, '../discrete_mixflows/')\n",
    "from discrete_mixflows import *\n",
    "from gibbs import *\n",
    "from concrete import *\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "plt.rcParams[\"figure.figsize\"]=15,7.5\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a095211",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "# target specification #\n",
    "########################\n",
    "########################\n",
    "np.random.seed(2023)\n",
    "K1=10\n",
    "prbs=np.random.rand(K1)\n",
    "prbs=prbs/np.sum(prbs)\n",
    "def lp(x,axis=None):\n",
    "    # compute the univariate log joint and conditional target pmfs\n",
    "    #\n",
    "    # inputs:\n",
    "    #    x    : (1,d) array with state values\n",
    "    #    axis : int, full conditional to calculate; returns joint if None\n",
    "    # outputs:\n",
    "    #   ext_lprb : if axis is None, (d,) array with log joint; else, (d,K1) array with d conditionals \n",
    "    \n",
    "    ext_lprb=np.log(np.repeat(prbs[:,np.newaxis],x.shape[1],axis=1).T)\n",
    "    if axis==None: return np.squeeze(ext_lprb[np.arange(0,x.shape[1]),x])\n",
    "    return ext_lprb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0225c",
   "metadata": {},
   "source": [
    "## IDF\n",
    "from https://github.com/jmtomczak/intro_dgm/blob/main/flows/idf_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28a0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae20b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chakraborty & Chakravarty, \"A new discrete probability distribution with integer support on (−∞, ∞)\",\n",
    "#  Communications in Statistics - Theory and Methods, 45:2, 492-505, DOI: 10.1080/03610926.2013.830743\n",
    "\n",
    "def log_min_exp(a, b, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "    Computes the log of exp(a) - exp(b) in a (more) numerically stable fashion.\n",
    "    Using:\n",
    "     log(exp(a) - exp(b))\n",
    "     c + log(exp(a-c) - exp(b-c))\n",
    "     a + log(1 - exp(b-a))\n",
    "    And note that we assume b < a always.\n",
    "    \"\"\"\n",
    "    y = a + torch.log(1 - torch.exp(b - a) + epsilon)\n",
    "\n",
    "    return y\n",
    "\n",
    "def log_integer_probability(x, mean, logscale):\n",
    "    scale = torch.exp(logscale)\n",
    "\n",
    "    logp = log_min_exp(\n",
    "        F.logsigmoid((x + 0.5 - mean) / scale),\n",
    "        F.logsigmoid((x - 0.5 - mean) / scale))\n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a79d52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jornpeters/integer_discrete_flows\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        rounded = torch.round(input, out=None)\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f236cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D):\n",
    "        super(IDF, self).__init__()\n",
    "        \n",
    "        if len(netts) == 1:\n",
    "            self.t = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "        else:\n",
    "            raise ValueError('You can provide either 1 or 4 translation nets.')\n",
    "        \n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        self.round = RoundStraightThrough.apply\n",
    "        \n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        self.D = 1\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        \n",
    "        xa=x\n",
    "        xb=x\n",
    "        if self.D>1: (xa, xb) = torch.chunk(x, 2, 0)\n",
    "        if forward:\n",
    "            yb = xb + self.round(self.t[index](xa))\n",
    "        else:\n",
    "            yb = xb - self.round(self.t[index](xa))\n",
    "        return torch.cat((xa, yb), 0)\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -self.log_prior(z).sum()\n",
    "        else:\n",
    "            return -self.log_prior(z).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        # sample z:\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D)\n",
    "        # x = f^-1(z)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        # Sample from logistic\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        # And then round it to an integer.\n",
    "        return torch.round(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c6e76f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIDF(depth,lprbs,layers=32):\n",
    "    dim=lprbs.shape[0]\n",
    "    dim=1\n",
    "    \n",
    "    inlayers=1 if dim==1 else dim//2\n",
    "    nett = lambda: nn.Sequential(\n",
    "        nn.Linear(inlayers, layers), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(layers, layers), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(layers, inlayers)\n",
    "    )\n",
    "    netts = [nett]\n",
    "\n",
    "    # Init IDF\n",
    "    return IDF(netts, depth, D=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "23a81d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIDF(depth,lprbs,layers=32,max_iters=1000,lr=1e-4,mc_ss=1000,seed=0,verbose=True):\n",
    "    \"\"\"\n",
    "    Train a RealNVP normalizing flow targeting lprbs using the Adam optimizer\n",
    "    \n",
    "    Input:\n",
    "        temp      : float, temperature of Concrete relaxation\n",
    "        depth     : int, number of couplings (transformations)\n",
    "        lprbs     : (dim,) array, target log probabilities\n",
    "        layers    : int, width of the linear layers\n",
    "        max_iters : int, max number of Adam iters\n",
    "        lr        : float, Adam learning rate\n",
    "        mc_ss     : int, number of samples to draw from target for training\n",
    "        seed      : int, for reproducinility\n",
    "        verbose   : boolean, indicating whether to print loss every 100 iterations of Adam\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # create flow\n",
    "    flow=createIDF(depth,lprbs,layers)\n",
    "    target = torch.distributions.categorical.Categorical(torch.tensor(np.exp(lprbs)))\n",
    "    \n",
    "    # train flow\n",
    "    sample=target.sample((mc_ss,1)).float()\n",
    "    optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=lr)\n",
    "    losses=np.zeros(max_iters)\n",
    "    \n",
    "    for t in range(max_iters):\n",
    "        loss = -flow.forward(sample).mean()\n",
    "        losses[t]=loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and t%(max_iters//10) == 0: print('iter %s:' % t, 'loss = %.3f' % loss)\n",
    "    # end for\n",
    "    return flow,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "33ec08c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_integer_probability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flow,losses\u001b[38;5;241m=\u001b[39m\u001b[43mtrainIDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlprbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmc_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [149], line 28\u001b[0m, in \u001b[0;36mtrainIDF\u001b[0;34m(depth, lprbs, layers, max_iters, lr, mc_ss, seed, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m losses\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(max_iters)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     29\u001b[0m     losses[t]\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn [147], line 55\u001b[0m, in \u001b[0;36mIDF.forward\u001b[0;34m(self, x, reduction)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior(z)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn [147], line 65\u001b[0m, in \u001b[0;36mIDF.log_prior\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prior\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 65\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m \u001b[43mlog_integer_probability\u001b[49m(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogscale)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_p\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_integer_probability' is not defined"
     ]
    }
   ],
   "source": [
    "flow,losses=trainIDF(depth=10,lprbs=np.log(prbs),layers=32,max_iters=1000,lr=1e-4,mc_ss=1000,seed=2023,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ba6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
