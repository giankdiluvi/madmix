{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4879d81",
   "metadata": {},
   "source": [
    "# Concrete GMM extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894557ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sys,time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "01842e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "####################\n",
    "#   auxiliary fns  #\n",
    "####################\n",
    "####################\n",
    "def concrete_gmm_flatten(ws,mus,Hs):\n",
    "    \"\"\"\n",
    "    Flatten weights, meand, and logCholeskys into 2D array\n",
    "    \n",
    "    Inputs:\n",
    "        ws  : (K,B) array, weights\n",
    "        mus : (K,D,B) array, cluster means\n",
    "        Hs  : (K,D,D,B) array, cluster logCholesky matrices\n",
    "    \n",
    "    Outpus:\n",
    "        xc  : (K',B) array, flattened values\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + Kx(D+DChoose2) (covariances)\n",
    "    \"\"\"\n",
    "    K,D,B=mus.shape\n",
    "    \n",
    "    flat_mus=mus.reshape(K*D,B)\n",
    "    idx=torch.tril_indices(D,D)\n",
    "    flat_Hs=Hs[:,idx[0],idx[1],:]                     # recover lower triangular entries\n",
    "    flat_Hs=flat_Hs.reshape(int(K*D*(1+0.5*(D-1))),B) # correct shape\n",
    "    return torch.vstack((ws,flat_mus,flat_Hs))\n",
    "\n",
    "\n",
    "def concrete_gmm_unflatten(xc,K,D):\n",
    "    \"\"\"\n",
    "    Unflatten xc into weights, meand, and covariances\n",
    "    \n",
    "    Inputs:\n",
    "        xc  : (K',B) array, flattened values\n",
    "    \n",
    "    Outputs:\n",
    "        ws  : (K,B) array, weights\n",
    "        mus : (K,D,B) array, cluster means\n",
    "        Hs  : (K,D,D,B) array, cluster logCholesky matrices\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'= K (weights) + KxD (means) + Kx(D+DChoose2) (covariances)\n",
    "    \"\"\"\n",
    "    B=xc.shape[-1]\n",
    "    \n",
    "    # recover each flattened var\n",
    "    ws=xc[:K,:]\n",
    "    flat_mus=xc[K:(K*D+K),:]\n",
    "    flat_Hs=xc[(K*D+K):,:].reshape(K,int(D*(1+0.5*(D-1))),B)\n",
    "    \n",
    "    # unflatten separately\n",
    "    mus=flat_mus.reshape(K,D,B)\n",
    "    Hs=torch.zeros((K,D,D,B))\n",
    "    idx=torch.tril_indices(D,D)\n",
    "    Hs[:,idx[0],idx[1],:]=flat_Hs\n",
    "    \n",
    "    return ws,mus,Hs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def HtoSigma(Hs):\n",
    "    \"\"\"\n",
    "    Transform logCholesky factors into covariance matrices\n",
    "    \n",
    "    Inputs:\n",
    "        Hs : (K,D,D,B) array, B observations of the K cluster logCholesky factors\n",
    "        \n",
    "    Outpus:\n",
    "        Sigmas : (K,D,D,B) array, B observations of the K cluster covariances\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=np.diag_indices(Hs.shape[1])\n",
    "    Ls=torch.clone(Hs)\n",
    "    Ls[:,idx[0],idx[1],:]=torch.exp(Hs[:,idx[0],idx[1],:])\n",
    "    Ls=torch.moveaxis(Ls,3,1) # so matrices are stacked in last two axes for matmul\n",
    "    #Sigmas=torch.matmul(Ls,torch.transpose(Ls,axes=(0,1,3,2)))\n",
    "    Sigmas=torch.matmul(Ls,torch.moveaxis(Ls,2,3))\n",
    "    return torch.moveaxis(Sigmas,1,3)\n",
    "\n",
    "def SigmatoH(Sigmas):\n",
    "    \"\"\"\n",
    "    Transform covariance matrices into logCholesky factors\n",
    "    \n",
    "    Inputs:\n",
    "        Sigmas : (K,D,D,B) array, B observations of the K cluster covariances\n",
    "        \n",
    "    Outpus:\n",
    "        Hs : (K,D,D,B) array, B observations of the K cluster logCholesky factors\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=np.diag_indices(Sigmas.shape[1])\n",
    "    Ls=torch.linalg.cholesky(torch.moveaxis(Sigmas,3,1))\n",
    "    Hs=torch.clone(Ls)\n",
    "    Hs[:,:,idx[0],idx[1]]=torch.log(Ls[:,:,idx[0],idx[1]])\n",
    "    return torch.moveaxis(Hs,1,3)\n",
    "\n",
    "\n",
    "def project_simplex_2d(v):\n",
    "        \"\"\"\n",
    "        Helper function, assuming that all vectors are arranged in rows of v.\n",
    "\n",
    "        :param v: NxD torch tensor; Duchi et al. algorithm is applied to each row in vectorized form\n",
    "        :return: w: result of the projection\n",
    "        \n",
    "        NOTE: taken from https://github.com/smatmo/ProjectionOntoSimplex/blob/master/project_simplex_pytorch.py\n",
    "        and modified by Gian Carlo Diluvi\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            shape = v.shape\n",
    "            if shape[1] == 1:\n",
    "                w = v.clone().detach()\n",
    "                w[:] = 1.\n",
    "                return w\n",
    "\n",
    "            mu = torch.sort(v, dim=1)[0]\n",
    "            mu = torch.flip(mu, dims=(1,))\n",
    "            cum_sum = torch.cumsum(mu, dim=1)\n",
    "            j = torch.unsqueeze(torch.arange(1, shape[1] + 1, dtype=mu.dtype, device=mu.device), 0)\n",
    "            rho = torch.sum(mu * j - cum_sum + 1. > 0.0, dim=1, keepdim=True) - 1\n",
    "            max_nn = cum_sum[torch.arange(shape[0]), rho[:, 0]]\n",
    "            theta = (torch.unsqueeze(max_nn, -1) - 1.) / (rho.type(max_nn.dtype) + 1)\n",
    "            w = torch.clamp(v - theta, min=0.0)\n",
    "            return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b0be8",
   "metadata": {},
   "source": [
    "## Gibbs Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9827fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 1000/1000\r"
     ]
    }
   ],
   "source": [
    "####################\n",
    "####################\n",
    "#  gibbs sampling  #\n",
    "####################\n",
    "####################\n",
    "def gibbs_gmm(y,mu0,sigma0,w0,steps,burnin_pct,seed=0):\n",
    "    \"\"\"\n",
    "    Run a Gibbs sampler for the labels x, weights w,\n",
    "    means mu, and covariance matrices sigma of\n",
    "    a Gaussian mixture model with obsevations\n",
    "    yn~sum_k wk Gaussian(muk,sigmak), n=1,...,N\n",
    "\n",
    "    Inputs:\n",
    "        y          : (N,d) array, observations (N is no. of obs, d is dimension of each obs)\n",
    "        mu0        : (K,d) array, initial means (K is number of clusters)\n",
    "        sigma0     : (K,d,d) array, initial covariances\n",
    "        w0         : (K,) array, initial weights\n",
    "        steps      : int, number of steps to run the sampler from (after burn-in)\n",
    "        burnin_pct : float, percentage of burn-in desired\n",
    "        seed       : int, random seed\n",
    "\n",
    "     Outputs:\n",
    "         xs        : (steps,N) array, labels samples\n",
    "         ws        : (steps,K) array, weights samples\n",
    "         mus       : (steps,K,d) array, means samples\n",
    "         sigmas    : (steps,K,d,d) array, covariance matrices samples\n",
    "\n",
    "    Note: the total number of steps the sampler is run for is\n",
    "          (T=steps+burn_in), where (burn_in=T*burnin_pct).\n",
    "          The total burn-in steps is therefore\n",
    "          (steps*burnin_pct/(1-burnin_pct))\n",
    "    \"\"\"\n",
    "    np.random.seed(0+seed)\n",
    "    \n",
    "    # get sizes, calculate steps\n",
    "    N,d=y.shape\n",
    "    K=mu0.shape[0]\n",
    "    burnin_steps=int(steps*burnin_pct/(1-burnin_pct))\n",
    "    total_steps=burnin_steps+steps+1\n",
    "    \n",
    "    # init params\n",
    "    xs=np.zeros((total_steps,N),dtype=int)\n",
    "    xs[0,:]=np.random.randint(low=0,high=K,size=N)\n",
    "    ws=np.ones((total_steps,K))/K\n",
    "    ws[0,:]=w0\n",
    "    mus=np.zeros((total_steps,K,d))\n",
    "    mus[0,:,:]=mu0\n",
    "    sigmas=np.ones((total_steps,K,d,d))\n",
    "    sigmas[0,:,:,:]=sigma0\n",
    "    \n",
    "    for t in range(total_steps-1):\n",
    "        if t<burnin_steps: print('Burn-in: '+str(t+1)+'/'+str(burnin_steps),end='\\r')\n",
    "        if t>=burnin_steps: print('Sampling: '+str(t+1-burnin_steps)+'/'+str(steps),end='\\r')\n",
    "        \n",
    "        # update indices ###\n",
    "        # first obtain log probabilities\n",
    "        tmplprbs=np.ones((N,K))*np.log(ws[t,:])\n",
    "        for k in range(K): tmplprbs[:,k]+=stats.multivariate_normal(mus[t,k,:],sigmas[t,k,:,:]).logpdf(y)\n",
    "        # then sample using gumbel-max trick\n",
    "        G=np.random.gumbel(size=(N,K))\n",
    "        tmpx=np.argmax(tmplprbs+G,axis=1)\n",
    "        xs[t+1,:]=tmpx\n",
    "        \n",
    "        # get cluster summaries\n",
    "        x_tuple=np.zeros((N,K),dtype=int)\n",
    "        x_tuple[np.arange(N),tmpx]=1\n",
    "        Nks=np.sum(x_tuple,axis=0)\n",
    "        \n",
    "        # update weights ###\n",
    "        tmpw=np.random.dirichlet(Nks+1)\n",
    "        ws[t+1,:]=tmpw\n",
    "        \n",
    "        # update means and covariances ###\n",
    "        for k in range(K):\n",
    "            yk=y[tmpx==k,:] # cluster elements, avg in next line\n",
    "            Nk=yk.shape[0]\n",
    "            \n",
    "            # update covariance\n",
    "            Sk=Nk*np.cov(yk,rowvar=False) # cluster covariance\n",
    "            tmpsigma=sigmas[t,k,:,:]\n",
    "            if np.linalg.cond(Sk) < 1/sys.float_info.epsilon: tmpsigma = stats.invwishart(Nk-d-1,Sk).rvs() # Sk invertible\n",
    "            sigmas[t+1,k,:,:]=tmpsigma\n",
    "            \n",
    "            # update mean\n",
    "            mus[t+1,k,:]=np.random.multivariate_normal(np.mean(yk,axis=0),sigmas[t+1,k,:,:]/Nk)       \n",
    "        # end for\n",
    "    # end for\n",
    "    burnin_steps+=1 # to account for initial draw\n",
    "    return xs[burnin_steps:,...],ws[burnin_steps:,...],mus[burnin_steps:,...],sigmas[burnin_steps:,...]\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "####################\n",
    "#  data wrangling  #\n",
    "####################\n",
    "####################\n",
    "of_dat=pd.read_table('https://gist.githubusercontent.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv')\n",
    "dat=np.array(of_dat)\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "####################\n",
    "#      setup       #\n",
    "####################\n",
    "####################\n",
    "\n",
    "# settings\n",
    "K=2\n",
    "steps=1000\n",
    "burnin_pct=0.5\n",
    "d=dat.shape[1]\n",
    "\n",
    "# initial arrays\n",
    "mu0=np.array([[2,50],[5,80]])\n",
    "sigma0=np.zeros((K,2,2))\n",
    "for k in range(K): sigma0[k,:,:]=5.*np.eye(d)\n",
    "w0=np.ones(K)/K\n",
    "\n",
    "\n",
    "####################\n",
    "####################\n",
    "#   run sampler    #\n",
    "####################\n",
    "####################\n",
    "xs,ws,mus,sigmas=gibbs_gmm(y=np.array(of_dat),mu0=mu0,sigma0=sigma0,w0=w0,steps=steps,burnin_pct=burnin_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9501da",
   "metadata": {},
   "source": [
    "## Concrete src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "62da6242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "########################################\n",
    "########################################\n",
    "expConcrete distribution\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "taken from\n",
    "https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_categorical.html#RelaxedOneHotCategorical\n",
    "since it is not imported with torch.distributions\n",
    "\"\"\"\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.utils import clamp_probs, broadcast_all\n",
    "from torch.distributions.distribution import Distribution\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import ExpTransform\n",
    "\n",
    "class ExpRelaxedCategorical(Distribution):\n",
    "    r\"\"\"\n",
    "    Creates a ExpRelaxedCategorical parameterized by\n",
    "    :attr:`temperature`, and either :attr:`probs` or :attr:`logits` (but not both).\n",
    "    Returns the log of a point in the simplex. Based on the interface to\n",
    "    :class:`OneHotCategorical`.\n",
    "\n",
    "    Implementation based on [1].\n",
    "\n",
    "    See also: :func:`torch.distributions.OneHotCategorical`\n",
    "\n",
    "    Args:\n",
    "        temperature (Tensor): relaxation temperature\n",
    "        probs (Tensor): event probabilities\n",
    "        logits (Tensor): unnormalized log probability for each event\n",
    "\n",
    "    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\n",
    "    (Maddison et al, 2017)\n",
    "\n",
    "    [2] Categorical Reparametrization with Gumbel-Softmax\n",
    "    (Jang et al, 2017)\n",
    "    \"\"\"\n",
    "    arg_constraints = {'probs': constraints.simplex,\n",
    "                       'logits': constraints.real_vector}\n",
    "    support = constraints.real_vector  # The true support is actually a submanifold of this.\n",
    "    has_rsample = True\n",
    "\n",
    "    def __init__(self, temperature, probs=None, logits=None, validate_args=None):\n",
    "        self._categorical = Categorical(probs, logits)\n",
    "        self.temperature = temperature\n",
    "        batch_shape = self._categorical.batch_shape\n",
    "        event_shape = self._categorical.param_shape[-1:]\n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(ExpRelaxedCategorical, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.temperature = self.temperature\n",
    "        new._categorical = self._categorical.expand(batch_shape)\n",
    "        super(ExpRelaxedCategorical, new).__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    def _new(self, *args, **kwargs):\n",
    "        return self._categorical._new(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def param_shape(self):\n",
    "        return self._categorical.param_shape\n",
    "\n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self._categorical.logits\n",
    "\n",
    "    @property\n",
    "    def probs(self):\n",
    "        return self._categorical.probs\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        uniforms = clamp_probs(torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device))\n",
    "        gumbels = -((-(uniforms.log())).log())\n",
    "        scores = (self.logits + gumbels) / self.temperature\n",
    "        return scores - scores.logsumexp(dim=-1, keepdim=True)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        K = self._categorical._num_events\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        logits, value = broadcast_all(self.logits, value)\n",
    "        log_scale = (torch.full_like(self.temperature, float(K)).lgamma() -\n",
    "                     self.temperature.log().mul(-(K - 1)))\n",
    "        score = logits - value.mul(self.temperature)\n",
    "        score = (score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)\n",
    "        return score + log_scale\n",
    "#========================================\n",
    "\n",
    "\"\"\"\n",
    "########################################\n",
    "########################################\n",
    "RealNVP architecture\n",
    "########################################\n",
    "########################################\n",
    "\"\"\"\n",
    "class RealNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Real Non Volume Preserving architecture\n",
    "    taken from\n",
    "    https://colab.research.google.com/github/senya-ashukha/real-nvp-pytorch/blob/master/real-nvp-pytorch.ipynb#scrollTo=p5YX9_EW3_EU\n",
    "    and adapted slightly\n",
    "    \"\"\"\n",
    "    def __init__(self, nets, nett, mask, prior, gmm=False):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(mask))])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(mask))])\n",
    "        self.gmm = gmm\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x*self.mask[i]\n",
    "            s = self.s[i](x_)*(1 - self.mask[i])\n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z\n",
    "            s = self.s[i](z_) * (1-self.mask[i])\n",
    "            t = self.t[i](z_) * (1-self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=-1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self,x):\n",
    "        z, logp = self.backward(x)\n",
    "        return self.prior.log_prob(z) + logp\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        size = (batchSize,) if self.gmm else (batchSize, 1)\n",
    "        z = self.prior.sample(size).float()\n",
    "        #logp = self.prior.log_prob(z)\n",
    "        x = self.forward(z)\n",
    "        return x\n",
    "\n",
    "#========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "459b972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "########################################\n",
    "########################################\n",
    "Gaussian mixture model training\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "taken from\n",
    "https://colab.research.google.com/github/senya-ashukha/real-nvp-pytorch/blob/master/real-nvp-pytorch.ipynb#scrollTo=p5YX9_EW3_EU\n",
    "and adapted slightly\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GMMRef(Distribution):\n",
    "    \"\"\"\n",
    "    Reference distribution for labels, weights, means, and covariances of a GMM.\n",
    "    Uninformative relaxed ExpConcrete for the labels,\n",
    "    Dirichlet for the weights,\n",
    "    Gaussians for the means,\n",
    "    and InverseWishart for the covariances\n",
    "\n",
    "    Inputs:\n",
    "        N    : int, number of observations from the GMM\n",
    "        K    : int, mixture size\n",
    "        tau0 : float, means prior precision\n",
    "        temp : float, temperature of Concrete relaxation\n",
    "    \"\"\"\n",
    "    def __init__(self, N,K,tau0=1.,temp=1.):\n",
    "        self.relcat = ExpRelaxedCategorical(torch.tensor([temp]),torch.ones(K)/K)\n",
    "        self.dirichlet = torch.distributions.dirichlet.Dirichlet(concentration=torch.ones(K))\n",
    "        self.gauss  = torch.distributions.MultivariateNormal(torch.zeros(K), torch.eye(K)/np.sqrt(tau0))\n",
    "        self.invwis = torch.distributions.wishart.Wishart(df=N/K,covariance_matrix=torch.eye(K))\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = 2 # for now only 2D data\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        relcat_lp = torch.zeros(value.shape[0])\n",
    "        for n in range(self.N): relcat_lp += self.relcat.log_prob(value[...,n*self.K+torch.arange(0,self.K)])\n",
    "            \n",
    "        idx=np.diag_indices(self.D)\n",
    "        xc=value[...,self.N*self.K:].T\n",
    "        ws,mus,Hs=concrete_gmm_unflatten(xc,self.K,self.D) #(K,B),(K,D,B),(K,D,D,B), D=2 ONLY FOR NOW\n",
    "        Sigmas=HtoSigma(Hs) #(K,D,D,B)\n",
    "        xc_lp=self.dirichlet.log_prob(project_simplex_2d(ws.T))\n",
    "        for k in range(self.K):\n",
    "            xc_lp += self.invwis.log_prob(torch.moveaxis(Sigmas[k,...],2,0))\n",
    "            xc_lp += self.D*torch.log(torch.tensor([2])) + torch.sum((self.D-torch.arange(self.D)+1)[:,None]*Hs[k,idx[0],idx[1],:],axis=0) # determinant Jacobian of log-Cholesky decomposition\n",
    "            xc_lp += self.gauss.log_prob(mus[k,...].T)\n",
    "        # end for\n",
    "        return relcat_lp+xc_lp\n",
    "\n",
    "    def sample(self,sample_shape=torch.Size()):\n",
    "        relcat_sample=self.relcat.sample(sample_shape)\n",
    "        for n in range(self.N-1): relcat_sample = torch.hstack((relcat_sample,self.relcat.sample(sample_shape)))\n",
    "        \n",
    "        ws_sample = self.dirichlet.sample(sample_shape).T\n",
    "        sigmas_sample = torch.zeros((self.K,self.D,self.D,sample_shape[0]))\n",
    "        mus_sample = torch.zeros((self.K,self.D,sample_shape[0]))\n",
    "        for k in range(self.K-1):\n",
    "            sigmas_sample[k,...] = torch.moveaxis(self.invwis.sample(sample_shape),0,2)\n",
    "            mus_sample[k,...] = self.gauss.sample(sample_shape).T\n",
    "        # end for\n",
    "        xc_sample = concrete_gmm_flatten(ws_sample,mus_sample,sigmas_sample).T\n",
    "        return torch.hstack((relcat_sample,xc_sample))\n",
    "#========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4ea35bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_concrete_sample(pred_x,pred_w,pred_mus,pred_sigmas,temp):\n",
    "    \"\"\"\n",
    "    Generate sample for learning GMM with RealNVP\n",
    "    using a Concrete relaxation for the labels\n",
    "\n",
    "    Inputs:\n",
    "        pred_x      : (steps,N) array, predicted label values (from `gibbs.gibbs_gmm`)\n",
    "        pred_w      : (steps,K) array, predicted weights (from `gibbs.gibbs_gmm`)\n",
    "        pred_mus    : (steps,K,D) array, predicted means (from `gibbs.gibbs_gmm`)\n",
    "        pred_sigmas : (steps,K,D,D) array, predicted covariance matrices (from `gibbs.gibbs_gmm`)\n",
    "        temp        : float, temperature of Concrete relaxation\n",
    "\n",
    "    Outputs:\n",
    "        conc_sample : (steps,K') array, samples to be used in training\n",
    "        \n",
    "    Note: K' = NK (labels) + K (weights) + KD (means) + Kx(D+DChoose2) (covariances, log-Cholesky decomposed)\n",
    "    \"\"\"\n",
    "    # estimate probabilities of each xn\n",
    "    pred_x=pred_x.T\n",
    "    x_prbs=torch.sum(pred_x==torch.arange(0,pred_mus.shape[1],dtype=int)[:,np.newaxis,np.newaxis],axis=-1)\n",
    "    x_prbs=(x_prbs/torch.sum(x_prbs,axis=0)[None,:]).T\n",
    "\n",
    "    # generate sample for training using Gibbs output and Gumbel soft-max\n",
    "    G=torch.from_numpy(np.random.gumbel(size=(pred_x.shape[-1],x_prbs.shape[0],x_prbs.shape[1])))\n",
    "    conc_sample=(x_prbs[np.newaxis,...]+G)/temp-torch.log(torch.sum(torch.exp((x_prbs[None,...]+G)/temp),axis=-1))[...,None]\n",
    "    conc_sample=conc_sample.reshape(pred_x.shape[-1],x_prbs.shape[0]*x_prbs.shape[1])\n",
    "    \n",
    "    # deal with continuous variables\n",
    "    Hs=SigmatoH(torch.moveaxis(pred_sigmas,0,3))\n",
    "    xc=madmix_gmm_flatten(pred_w.T,torch.moveaxis(pred_mus,0,2),Hs).T\n",
    "    \n",
    "    # merge everything\n",
    "    conc_sample=torch.hstack((conc_sample,torch.from_numpy(xc))).float()\n",
    "\n",
    "    return conc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a36ac843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGMMRealNVP(temp,depth,N,K,tau0,width=32):\n",
    "    \"\"\"\n",
    "    Wrapper to init a RealNVP class for a GMM problem\n",
    "\n",
    "    The reference distribution is a relaxed uniform in exp space\n",
    "    for the labels and a multivariate normal with precision tau0\n",
    "    for the means\n",
    "\n",
    "    Inputs:\n",
    "        temp   : float, temperature of Concrete relaxation\n",
    "        depth  : int, number of couplings (transformations)\n",
    "        N      : int, number of observations from the GMM\n",
    "        K      : int, mixture size\n",
    "        tau0   : float, prior precision for means\n",
    "        width : int, width of the linear layers\n",
    "\n",
    "    Outputs:\n",
    "        flow   : Module, RealNVP\n",
    "    \"\"\"\n",
    "    D=2 # currently only 2D examples supported\n",
    "    dim=int(K*N+K+K*D+K*(D+D*(D-1)/2))\n",
    "\n",
    "    # create channel-wise masks of appropriate size\n",
    "    masks=torch.zeros((2,dim))\n",
    "    masks[0,:(dim//2)]=1\n",
    "    masks[1,(dim-(dim//2)):]=1\n",
    "    masks=masks.repeat(depth//2,1)\n",
    "\n",
    "    # define reference distribution\n",
    "    ref = GMMRef(N,K,tau0,temp)\n",
    "\n",
    "    # define scale and translation architectures\n",
    "    net_s = lambda: nn.Sequential(\n",
    "        nn.Linear(dim, width),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(width, width),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(width, dim),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    net_t = lambda: nn.Sequential(\n",
    "        nn.Linear(dim, width),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(width, width),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(width, dim)\n",
    "    )\n",
    "    return RealNVP(net_s, net_t, masks, ref, gmm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f9b8aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGMMRealNVP(temp,depth,N,K,tau0,sample,width=32,max_iters=1000,lr=1e-4,seed=0,verbose=True):\n",
    "    \"\"\"\n",
    "    Train a RealNVP normalizing flow targeting lprbs using the Adam optimizer\n",
    "\n",
    "    Input:\n",
    "        temp      : float, temperature of Concrete relaxation\n",
    "        depth     : int, number of couplings (transformations)\n",
    "        N         : int, number of observations from the GMM\n",
    "        K         : int, mixture size\n",
    "        tau0      : float, prior precision for means\n",
    "        sample    : (B,K*(N+1)) array, samples from target for training; B is the Monte Carlo sample size\n",
    "        width     : int, width of the linear layers\n",
    "        max_iters : int, max number of Adam iters\n",
    "        lr        : float, Adam learning rate\n",
    "        seed      : int, for reproducinility\n",
    "        verbose   : boolean, indicating whether to print loss every 100 iterations of Adam\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # create flow\n",
    "    flow=createGMMRealNVP(temp,depth,N,K,tau0,width)\n",
    "\n",
    "    # train flow\n",
    "    optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=lr)\n",
    "    losses=np.zeros(max_iters)\n",
    "\n",
    "    for t in range(max_iters):\n",
    "        loss = -flow.log_prob(sample).mean()\n",
    "        losses[t]=loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose and t%(max_iters//10) == 0: print('iter %s:' % t, 'loss = %.3f' % loss)\n",
    "    # end for\n",
    "    return flow,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a92389",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f9b720b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "temp=0.5\n",
    "depth=15\n",
    "width=16\n",
    "max_iters=10\n",
    "lr=1e-5\n",
    "N=dat.shape[0]\n",
    "K=2\n",
    "tau0=1.\n",
    "\n",
    "# convert gibbs output to torch tensors\n",
    "xs_concrete = torch.from_numpy(xs)\n",
    "ws_concrete = torch.from_numpy(ws)\n",
    "mus_concrete = torch.from_numpy(mus)\n",
    "sigmas_concrete = torch.from_numpy(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5dba7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_flow=createGMMRealNVP(temp,depth,N,K,tau0,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ba2e2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txd,tw,tmu,tH=full_concrete_gmm_unflatten(tt_flow.sample(10),N,K,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "eb6ccc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.4743867e-03 9.5872319e-01 6.6480833e-01 ... 4.7545318e-02\n",
      "   1.3654418e-03 5.2470643e-02]\n",
      "  [9.9452567e-01 4.1276917e-02 3.3519197e-01 ... 9.5245469e-01\n",
      "   9.9863434e-01 9.4752932e-01]]\n",
      "\n",
      " [[1.0000000e+00 6.6242740e-03 9.9868762e-01 ... 9.6682554e-01\n",
      "   1.6204904e-04 2.3901380e-04]\n",
      "  [0.0000000e+00 9.9337566e-01 1.3125829e-03 ... 3.3174425e-02\n",
      "   9.9983788e-01 9.9976099e-01]]\n",
      "\n",
      " [[1.0000000e+00 9.9885941e-01 1.0000000e+00 ... 1.9469692e-01\n",
      "   1.0000000e+00 9.9999809e-01]\n",
      "  [3.5496797e-22 1.1406186e-03 3.3650227e-14 ... 8.0530310e-01\n",
      "   1.7237043e-11 2.0820628e-06]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.6765711e-05 3.7862999e-06 4.2008463e-02 ... 1.7567578e-01\n",
      "   6.4316708e-01 3.4766082e-02]\n",
      "  [9.9997330e-01 9.9999619e-01 9.5799160e-01 ... 8.2432425e-01\n",
      "   3.5683295e-01 9.6523392e-01]]\n",
      "\n",
      " [[1.0000000e+00 5.3516819e-06 3.3086225e-01 ... 3.0801031e-05\n",
      "   3.3200601e-01 8.3060890e-08]\n",
      "  [6.8309170e-08 9.9999464e-01 6.6913766e-01 ... 9.9996924e-01\n",
      "   6.6799408e-01 9.9999988e-01]]\n",
      "\n",
      " [[4.7111431e-01 2.0718957e-01 1.9295496e-01 ... 9.3122178e-01\n",
      "   7.9275392e-02 9.1903514e-01]\n",
      "  [5.2888572e-01 7.9281044e-01 8.0704498e-01 ... 6.8778165e-02\n",
      "   9.2072463e-01 8.0964878e-02]]]\n"
     ]
    }
   ],
   "source": [
    "txd=txd.detach().numpy()\n",
    "txd=txd-LogSumExpv2(np.moveaxis(txd,1,0))[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "aefa814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_concrete_gmm_unflatten(x,N,K,D):\n",
    "    \"\"\"\n",
    "    Unflatten x=[xd,xc] into labels,weights, meand, and covariances\n",
    "    \n",
    "    Inputs:\n",
    "        x   : (B,K'') array, flattened values\n",
    "    \n",
    "    Outputs:\n",
    "        xd  : (N,K,B) array, labels\n",
    "        ws  : (K,B) array, weights\n",
    "        mus : (K,D,B) array, cluster means\n",
    "        Hs  : (K,D,D,B) array, cluster logCholesky matrices\n",
    "        \n",
    "    Note:\n",
    "    K is the number of clusters, D is data dimension, \n",
    "    and B is the number of data points (for vectorizing)\n",
    "    K'' = NK (labels) + K (weights) + KxD (means) + Kx(D+DChoose2) (covariances)\n",
    "    \"\"\"\n",
    "    x=x.T\n",
    "    B=x.shape[1]\n",
    "    \n",
    "    xd=x[:(N*K),:].reshape(N,K,B)\n",
    "    xc=x[(N*K):,:]\n",
    "    ws,mus,Hs=concrete_gmm_unflatten(xc,K,D)\n",
    "    \n",
    "    return xd,ws,mus,Hs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "199b7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Concrete sample\n",
    "conc_sample=gmm_concrete_sample(xs_concrete,ws_concrete,mus_concrete,sigmas_concrete,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f641c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = nan\n",
      "iter 1: loss = nan\n",
      "iter 2: loss = nan\n",
      "iter 3: loss = nan\n",
      "iter 4: loss = nan\n",
      "iter 5: loss = nan\n",
      "iter 6: loss = nan\n",
      "iter 7: loss = nan\n",
      "iter 8: loss = nan\n",
      "iter 9: loss = nan\n"
     ]
    }
   ],
   "source": [
    "tmp_flow,tmp_loss=trainGMMRealNVP(\n",
    "    temp=temp,depth=depth,N=N,K=K,tau0=tau0,sample=conc_sample,width=width,max_iters=max_iters,lr=lr,seed=2023,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "388f019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 556])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_flow.sample(1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
